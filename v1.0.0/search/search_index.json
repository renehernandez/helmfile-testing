{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Helmfile \u00b6 Deploy Kubernetes Helm Charts Status \u00b6 Even though Helmfile is used in production environments across multiple organizations , it is still in its early stage of development, hence versioned 0.x. Helmfile complies to Semantic Versioning 2.0.0 in which v0.x means that there could be backward-incompatible changes for every release. Note that we will try our best to document any backward incompatibility. And in reality, helmfile had no breaking change for a year or so. About \u00b6 Helmfile is a declarative spec for deploying helm charts. It lets you\u2026 Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm , the helmfile executable delegates to helm - as a result, helm must be installed. Highlights \u00b6 Declarative : Write, version-control, apply the desired state file for visibility and reproducibility. Modules : Modularize common patterns of your infrastructure, distribute it via Git, S3, etc. to be reused across the entire company (See #648 ) Versatility : Manage your cluster consisting of charts, kustomizations , and directories of Kubernetes resources, turning everything to Helm releases (See #673 ) Patch : JSON/Strategic-Merge Patch Kubernetes resources before helm-install ing, without forking upstream charts (See #673 ) Configuration \u00b6 CAUTION : This documentation is for the development version of Helmfile. If you are looking for the documentation for any of releases, please switch to the corresponding release tag like v0.92.1 . The default name for a helmfile is helmfile.yaml : # Chart repositories used from within this state file # # Use `helm-s3` and `helm-git` and whatever Helm Downloader plugins # to use repositories other than the official repository or one backend by chartmuseum. repositories: # To use official \"stable\" charts a.k.a https://github.com/helm/charts/tree/master/stable - name: stable url: https://charts.helm.sh/stable # To use official \"incubator\" charts a.k.a https://github.com/helm/charts/tree/master/incubator - name: incubator url: https://charts.helm.sh/incubator # helm-git powered repository: You can treat any Git repository as a charts repository - name: polaris url: git+https://github.com/reactiveops/polaris@deploy/helm?ref=master # Advanced configuration: You can setup basic or tls auth and optionally enable helm OCI integration - name: roboll url: http://roboll.io/charts certFile: optional_client_cert keyFile: optional_client_key username: optional_username password: optional_password oci: true # Advanced configuration: You can use a ca bundle to use an https repo # with a self-signed certificate - name: insecure url: https://charts.my-insecure-domain.com caFile: optional_ca_crt # context: kube-context # this directive is deprecated, please consider using helmDefaults.kubeContext # Path to alternative helm binary (--helm-binary) helmBinary: path/to/helm3 # Default values to set for args along with dedicated keys that can be set by contributors, cli args take precedence over these. # In other words, unset values results in no flags passed to helm. # See the helm usage (helm SUBCOMMAND -h) for more info on default values when those flags aren't provided. helmDefaults: tillerNamespace: tiller-namespace #dedicated default key for tiller-namespace tillerless: false #dedicated default key for tillerless kubeContext: kube-context #dedicated default key for kube-context (--kube-context) cleanupOnFail: false #dedicated default key for helm flag --cleanup-on-fail # additional and global args passed to helm (default \"\") args: - \"--set k=v\" # verify the chart before upgrading (only works with packaged charts not directories) (default false) verify: true # wait for k8s resources via --wait. (default false) wait: true # if set and --wait enabled, will wait until all Jobs have been completed before marking the release as successful. It will wait for as long as --timeout (default false, Implemented in Helm3.5) waitForJobs: true # time in seconds to wait for any individual Kubernetes operation (like Jobs for hooks, and waits on pod/pvc/svc/deployment readiness) (default 300) timeout: 600 # performs pods restart for the resource if applicable (default false) recreatePods: true # forces resource update through delete/recreate if needed (default false) force: false # enable TLS for request to Tiller (default false) tls: true # path to TLS CA certificate file (default \"$HELM_HOME/ca.pem\") tlsCACert: \"path/to/ca.pem\" # path to TLS certificate file (default \"$HELM_HOME/cert.pem\") tlsCert: \"path/to/cert.pem\" # path to TLS key file (default \"$HELM_HOME/key.pem\") tlsKey: \"path/to/key.pem\" # limit the maximum number of revisions saved per release. Use 0 for no limit. (default 10) historyMax: 10 # when using helm 3.2+, automatically create release namespaces if they do not exist (default true) createNamespace: true # if used with charts museum allows to pull unstable charts for deployment, for example: if 1.2.3 and 1.2.4-dev versions exist and set to true, 1.2.4-dev will be pulled (default false) devel: true # When set to `true`, skips running `helm dep up` and `helm dep build` on this release's chart. # Useful when the chart is broken, like seen in https://github.com/roboll/helmfile/issues/1547 skipDeps: false # these labels will be applied to all releases in a Helmfile. Useful in templating if you have a helmfile per environment or customer and don't want to copy the same label to each release commonLabels: hello: world # The desired states of Helm releases. # # Helmfile runs various helm commands to converge the current state in the live cluster to the desired state defined here. releases: # Published chart example - name: vault # name of this release namespace: vault # target namespace createNamespace: true # helm 3.2+ automatically create release namespace (default true) labels: # Arbitrary key value pairs for filtering releases foo: bar chart: roboll/vault-secret-manager # the chart being installed to create this release, referenced by `repository/chart` syntax version: ~1.24.1 # the semver of the chart. range constraint is supported condition: vault.enabled # The values lookup key for filtering releases. Corresponds to the boolean value of `vault.enabled`, where `vault` is an arbitrary value missingFileHandler: Warn # set to either \"Error\" or \"Warn\". \"Error\" instructs helmfile to fail when unable to find a values or secrets file. When \"Warn\", it prints the file and continues. # Values files used for rendering the chart values: # Value files passed via --values - vault.yaml # Inline values, passed via a temporary values file and --values, so that it doesn't suffer from type issues like --set - address: https://vault.example.com # Go template available in inline values and values files. - image: # The end result is more or less YAML. So do `quote` to prevent number-like strings from accidentally parsed into numbers! # See https://github.com/roboll/helmfile/issues/608 tag: {{ requiredEnv \"IMAGE_TAG\" | quote }} # Otherwise: # tag: \"{{ requiredEnv \"IMAGE_TAG\" }}\" # tag: !!string {{ requiredEnv \"IMAGE_TAG\" }} db: username: {{ requiredEnv \"DB_USERNAME\" }} # value taken from environment variable. Quotes are necessary. Will throw an error if the environment variable is not set. $DB_PASSWORD needs to be set in the calling environment ex: export DB_PASSWORD='password1' password: {{ requiredEnv \"DB_PASSWORD\" }} proxy: # Interpolate environment variable with a fixed string domain: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com scheme: {{ env \"SCHEME\" | default \"https\" }} # Use `values` whenever possible! # `set` translates to helm's `--set key=val`, that is known to suffer from type issues like https://github.com/roboll/helmfile/issues/608 set: # single value loaded from a local file, translates to --set-file foo.config=path/to/file - name: foo.config file: path/to/file # set a single array value in an array, translates to --set bar[0]={1,2} - name: bar[0] values: - 1 - 2 # set a templated value - name: namespace value: {{ .Namespace }} # will attempt to decrypt it using helm-secrets plugin secrets: - vault_secret.yaml # Override helmDefaults options for verify, wait, waitForJobs, timeout, recreatePods and force. verify: true wait: true waitForJobs: true timeout: 60 recreatePods: true force: false # set `false` to uninstall this release on sync. (default true) installed: true # restores previous state in case of failed release (default false) atomic: true # when true, cleans up any new resources created during a failed release (default false) cleanupOnFail: false # name of the tiller namespace (default \"\") tillerNamespace: vault # if true, will use the helm-tiller plugin (default false) tillerless: false # enable TLS for request to Tiller (default false) tls: true # path to TLS CA certificate file (default \"$HELM_HOME/ca.pem\") tlsCACert: \"path/to/ca.pem\" # path to TLS certificate file (default \"$HELM_HOME/cert.pem\") tlsCert: \"path/to/cert.pem\" # path to TLS key file (default \"$HELM_HOME/key.pem\") tlsKey: \"path/to/key.pem\" # --kube-context to be passed to helm commands # CAUTION: this doesn't work as expected for `tilerless: true`. # See https://github.com/roboll/helmfile/issues/642 # (default \"\", which means the standard kubeconfig, either ~/kubeconfig or the file pointed by $KUBECONFIG environment variable) kubeContext: kube-context # passes --disable-validation to helm 3 diff plugin, this requires diff plugin >= 3.1.2 # It may be helpful to deploy charts with helm api v1 CRDS # https://github.com/roboll/helmfile/pull/1373 disableValidation: false # passes --disable-validation to helm 3 diff plugin, this requires diff plugin >= 3.1.2 # It is useful when any release contains custom resources for CRDs that is not yet installed onto the cluster. # https://github.com/roboll/helmfile/pull/1618 disableValidationOnInstall: false # passes --disable-openapi-validation to helm 3 diff plugin, this requires diff plugin >= 3.1.2 # It may be helpful to deploy charts with helm api v1 CRDS # https://github.com/roboll/helmfile/pull/1373 disableOpenApiValidation: false # limit the maximum number of revisions saved per release. Use 0 for no limit (default 10) historyMax: 10 # When set to `true`, skips running `helm dep up` and `helm dep build` on this release's chart. # Useful when the chart is broken, like seen in https://github.com/roboll/helmfile/issues/1547 skipDeps: false # Local chart example - name: grafana # name of this release namespace: another # target namespace chart: ../my-charts/grafana # the chart being installed to create this release, referenced by relative path to local helmfile values: - \"../../my-values/grafana/values.yaml\" # Values file (relative path to manifest) - ./values/{{ requiredEnv \"PLATFORM_ENV\" }}/config.yaml # Values file taken from path with environment variable. $PLATFORM_ENV must be set in the calling environment. wait: true # # Advanced Configuration: Nested States # helmfiles: - # Path to the helmfile state file being processed BEFORE releases in this state file path: path/to/subhelmfile.yaml # Label selector used for filtering releases in the nested state. # For example, `name=prometheus` in this context is equivalent to processing the nested state like # helmfile -f path/to/subhelmfile.yaml -l name=prometheus sync selectors: - name=prometheus # Override state values values: # Values files merged into the nested state's values - additional.values.yaml # One important aspect of using values here is that they first need to be defined in the values section # of the origin helmfile, so in this example key1 needs to be in the values or environments.NAME.values of path/to/subhelmfile.yaml # Inline state values merged into the nested state's values - key1: val1 - # All the nested state files under `helmfiles:` is processed in the order of definition. # So it can be used for preparation for your main `releases`. An example would be creating CRDs required by `releases` in the parent state file. path: path/to/mycrd.helmfile.yaml - # Terraform-module-like URL for importing a remote directory and use a file in it as a nested-state file # The nested-state file is locally checked-out along with the remote directory containing it. # Therefore all the local paths in the file are resolved relative to the file path: git::https://github.com/cloudposse/helmfiles.git@releases/kiam.yaml?ref=0.40.0 # If set to \"Error\", return an error when a subhelmfile points to a # non-existent path. The default behavior is to print a warning and continue. missingFileHandler: Error # # Advanced Configuration: Environments # # The list of environments managed by helmfile. # # The default is `environments: {\"default\": {}}` which implies: # # - `{{ .Environment.Name }}` evaluates to \"default\" # - `{{ .Values }}` being empty environments: # The \"default\" environment is available and used when `helmfile` is run without `--environment NAME`. default: # Everything from the values.yaml is available via `{{ .Values.KEY }}`. # Suppose `{\"foo\": {\"bar\": 1}}` contained in the values.yaml below, # `{{ .Values.foo.bar }}` is evaluated to `1`. values: - environments/default/values.yaml # Each entry in values can be either a file path or inline values. # The below is an example of inline values, which is merged to the `.Values` - myChartVer: 1.0.0-dev # Any environment other than `default` is used only when `helmfile` is run with `--environment NAME`. # That is, the \"production\" env below is used when and only when it is run like `helmfile --environment production sync`. production: values: - environment/production/values.yaml - myChartVer: 1.0.0 # disable vault release processing - vault: enabled: false ## `secrets.yaml` is decrypted by `helm-secrets` and available via `{{ .Environment.Values.KEY }}` secrets: - environment/production/secrets.yaml # Instructs helmfile to fail when unable to find a environment values file listed under `environments.NAME.values`. # # Possible values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\". # # Use \"Warn\", \"Info\", or \"Debug\" if you want helmfile to not fail when a values file is missing, while just leaving # a message about the missing file at the log-level. missingFileHandler: Error # kubeContext to use for this environment kubeContext: kube-context # # Advanced Configuration: Layering # # Helmfile merges all the \"base\" state files and this state file before processing. # # Assuming this state file is named `helmfile.yaml`, all the files are merged in the order of: # environments.yaml <- defaults.yaml <- templates.yaml <- helmfile.yaml bases: - environments.yaml - defaults.yaml - templates.yaml # # Advanced Configuration: API Capabilities # # 'helmfile template' renders releases locally without querying an actual cluster, # and in this case `.Capabilities.APIVersions` cannot be populated. # When a chart queries for a specific CRD, this can lead to unexpected results. # # Configure a fixed list of api versions to pass to 'helm template' via the --api-versions flag: apiVersions: - example/v1 Templating \u00b6 Helmfile uses Go templates for templating your helmfile.yaml. While go ships several built-in functions, we have added all of the functions in the Sprig library . We also added the following functions: requiredEnv exec readFile toYaml fromYaml setValueAtPath get (Sprig\u2019s original get is available as sprigGet ) tpl required fetchSecretValue expandSecretRefs We also added one special template function: requiredEnv . The requiredEnv function allows you to declare a particular environment variable as required for template rendering. If the environment variable is unset or empty, the template rendering will fail with an error message. Using environment variables \u00b6 Environment variables can be used in most places for templating the helmfile. Currently this is supported for name , namespace , value (in set), values and url (in repositories). Examples: repositories: - name: your-private-git-repo-hosted-charts url: https://{{ requiredEnv \"GITHUB_TOKEN\"}}@raw.githubusercontent.com/kmzfs/helm-repo-in-github/master/ releases: - name: {{ requiredEnv \"NAME\" }}-vault namespace: {{ requiredEnv \"NAME\" }} chart: roboll/vault-secret-manager values: - db: username: {{ requiredEnv \"DB_USERNAME\" }} password: {{ requiredEnv \"DB_PASSWORD\" }} set: - name: proxy.domain value: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com - name: proxy.scheme value: {{ env \"SCHEME\" | default \"https\" }} Note \u00b6 If you wish to treat your enviroment variables as strings always, even if they are boolean or numeric values you can use {{ env \"ENV_NAME\" | quote }} or \"{{ env \"ENV_NAME\" }}\" . These approaches also work with requiredEnv . Installation \u00b6 download one of releases or run as a container or Archlinux: install via pacman -S helmfile or from AUR or openSUSE: install via zypper in helmfile assuming you are on Tumbleweed; if you are on Leap you must add the kubic repo for your distribution version once before that command, e.g. zypper ar https://download.opensuse.org/repositories/devel:/kubic/openSUSE_Leap_\\$releasever kubic , or Windows (using scoop ): scoop install helmfile macOS (using homebrew ): brew install helmfile Running as a container \u00b6 The Helmfile Docker images are available in Quay . There is no latest tag, since the 0.x versions can contain breaking changes, so make sure you pick the right tag. Example using helmfile 0.135.0 : # helm 2 $ docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.helm:/root/.helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:v0.135.0 helmfile sync # helm 3 $ docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.config/helm:/root/.config/helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:helm3-v0.135.0 helmfile sync You can also use shims to make calling the binaries easier: # helm 2 $ printf '%s\\n' '#!/bin/sh' 'docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.helm:/root/.helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:v0.135.0 helmfile \"$@\"' | tee helmfile $ chmod +x helmfile $ ./helmfile sync # helm 3 $ printf '%s\\n' '#!/bin/sh' 'docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.config/helm:/root/.config/helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:helm3-v0.135.0 helmfile \"$@\"' | tee helmfile $ chmod +x helmfile $ ./helmfile sync Getting Started \u00b6 Let\u2019s start with a simple helmfile and gradually improve it to fit your use-case! Suppose the helmfile.yaml representing the desired state of your helm releases looks like: releases: - name: prom-norbac-ubuntu namespace: prometheus chart: stable/prometheus set: - name: rbac.create value: false Sync your Kubernetes cluster state to the desired one by running: helmfile apply Congratulations! You now have your first Prometheus deployment running inside your cluster. Iterate on the helmfile.yaml by referencing: Configuration CLI reference . Helmfile Best Practices Guide CLI Reference \u00b6 NAME: helmfile USAGE: helmfile [global options] command [command options] [arguments...] VERSION: v0.138.6 COMMANDS: deps update charts based on their requirements repos sync repositories from state file (helm repo add && helm repo update) charts DEPRECATED: sync releases from state file (helm upgrade --install) diff diff releases from state file against env (helm diff) template template releases from state file against env (helm template) write-values write values files for releases. Similar to `helmfile template`, write values files instead of manifests. lint lint charts from state file (helm lint) sync sync all resources from state file (repos, releases and chart deps) apply apply all resources from state file only when there are changes status retrieve status of releases in state file delete DEPRECATED: delete releases from state file (helm delete) destroy deletes and then purges releases test test releases from state file (helm test) build output compiled helmfile state(s) as YAML list list releases defined in state file fetch fetch charts from state file version Show the version for Helmfile. help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --helm-binary value, -b value path to helm binary (default: \"helm\") --file helmfile.yaml, -f helmfile.yaml load config from file or directory. defaults to helmfile.yaml or `helmfile.d`(means `helmfile.d/*.yaml`) in this preference --environment value, -e value specify the environment name. defaults to \"default\" --state-values-set value set state values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --state-values-file value specify state values in a YAML file --quiet, -q Silence output. Equivalent to log-level warn --kube-context value Set kubectl context. Uses current context by default --debug Enable verbose output for Helm and set log-level to debug, this disables --quiet/-q effect --no-color Output without color --log-level value Set log level, default info --namespace value, -n value Set namespace. Uses the namespace set in the context by default, and is available in templates as {{ .Namespace }} --selector value, -l value Only run using the releases that match labels. Labels can take the form of foo=bar or foo!=bar. A release must match all labels in a group in order to be used. Multiple groups can be specified at once. --selector tier=frontend,tier!=proxy --selector tier=backend. Will match all frontend, non-proxy releases AND all backend releases. The name of a release can be used as a label. --selector name=myrelease --allow-no-matching-release Do not exit with an error code if the provided selector has no matching releases. --interactive, -i Request confirmation before attempting to modify clusters --help, -h show help --version, -v print the version sync \u00b6 The helmfile sync sub-command sync your cluster state as described in your helmfile . The default helmfile is helmfile.yaml , but any YAML file can be passed by specifying a --file path/to/your/yaml/file flag. Under the covers, Helmfile executes helm upgrade --install for each release declared in the manifest, by optionally decrypting secrets to be consumed as helm chart values. It also updates specified chart repositories and updates the dependencies of any referenced local charts. For Helm 2.9+ you can use a username and password to authenticate to a remote repository. deps \u00b6 The helmfile deps sub-command locks your helmfile state and local charts dependencies. It basically runs helm dependency update on your helmfile state file and all the referenced local charts, so that you get a \u201clock\u201d file per each helmfile state or local chart. All the other helmfile sub-commands like sync use chart versions recorded in the lock files, so that e.g. untested chart versions won\u2019t suddenly get deployed to the production environment. For example, the lock file for a helmfile state file named helmfile.1.yaml will be helmfile.1.lock . The lock file for a local chart would be requirements.lock , which is the same as helm . It is recommended to version-control all the lock files, so that they can be used in the production deployment pipeline for extra reproducibility. To bring in chart updates systematically, it would also be a good idea to run helmfile deps regularly, test it, and then update the lock files in the version-control system. diff \u00b6 The helmfile diff sub-command executes the helm-diff plugin across all of the charts/releases defined in the manifest. To supply the diff functionality Helmfile needs the helm-diff plugin v2.9.0+1 or greater installed. For Helm 2.3+ you should be able to simply execute helm plugin install https://github.com/databus23/helm-diff . For more details please look at their documentation . apply \u00b6 The helmfile apply sub-command begins by executing diff . If diff finds that there is any changes, sync is executed. Adding --interactive instructs Helmfile to request your confirmation before sync . An expected use-case of apply is to schedule it to run periodically, so that you can auto-fix skews between the desired and the current state of your apps running on Kubernetes clusters. destroy \u00b6 The helmfile destroy sub-command uninstalls and purges all the releases defined in the manifests. helmfile --interactive destroy instructs Helmfile to request your confirmation before actually deleting releases. destroy basically runs helm uninstall --purge on all the targeted releases. If you don\u2019t want purging, use helmfile delete instead. delete (DEPRECATED) \u00b6 The helmfile delete sub-command deletes all the releases defined in the manifests. helmfile --interactive delete instructs Helmfile to request your confirmation before actually deleting releases. Note that delete doesn\u2019t purge releases. So helmfile delete && helmfile sync results in sync failed due to that releases names are not deleted but preserved for future references. If you really want to remove releases for reuse, add --purge flag to run it like helmfile delete --purge . secrets \u00b6 The secrets parameter in a helmfile.yaml causes the helm-secrets plugin to be executed to decrypt the file. To supply the secret functionality Helmfile needs the helm secrets plugin installed. For Helm 2.3+ you should be able to simply execute helm plugin install https://github.com/jkroepke/helm-secrets . test \u00b6 The helmfile test sub-command runs a helm test against specified releases in the manifest, default to all Use --cleanup to delete pods upon completion. lint \u00b6 The helmfile lint sub-command runs a helm lint across all of the charts/releases defined in the manifest. Non local charts will be fetched into a temporary folder which will be deleted once the task is completed. fetch \u00b6 The helmfile fetch sub-command downloads or copies local charts to a local directory for debug purpose. The local directory must be specified with --output-dir . Paths Overview \u00b6 Using manifest files in conjunction with command line argument can be a bit confusing. A few rules to clear up this ambiguity: Absolute paths are always resolved as absolute paths Relative paths referenced in the Helmfile manifest itself are relative to that manifest Relative paths referenced on the command line are relative to the current working directory the user is in For additional context, take a look at paths examples . Labels Overview \u00b6 A selector can be used to only target a subset of releases when running Helmfile. This is useful for large helmfiles with releases that are logically grouped together. Labels are simple key value pairs that are an optional field of the release spec. When selecting by label, the search can be inverted. tier!=backend would match all releases that do NOT have the tier: backend label. tier=fronted would only match releases with the tier: frontend label. Multiple labels can be specified using , as a separator. A release must match all selectors in order to be selected for the final helm command. The selector parameter can be specified multiple times. Each parameter is resolved independently so a release that matches any parameter will be used. --selector tier=frontend --selector tier=backend will select all the charts. In addition to user supplied labels, the name, the namespace, and the chart are available to be used as selectors. The chart will just be the chart name excluding the repository (Example stable/filebeat would be selected using --selector chart=filebeat ). commonLabels can be used when you want to apply the same label to all releases and use templating based on that. For instance, you install a number of charts on every customer but need to provide different values file per customer. templates/common.yaml: templates: nginx: &nginx name: nginx chart: stable/nginx-ingress values: - ../values/common/{{ .Release.Name }}.yaml - ../values/{{ .Release.Labels.customer }}/{{ .Release.Name }}.yaml cert-manager: &cert-manager name: cert-manager chart: jetstack/cert-manager values: - ../values/common/{{ .Release.Name }}.yaml - ../values/{{ .Release.Labels.customer }}/{{ .Release.Name }}.yaml helmfile.yaml: {{ readFile \"templates/common.yaml\" }} commonLabels: customer: company releases: - <<: *nginx - <<: *cert-manager Templates \u00b6 You can use go\u2019s text/template expressions in helmfile.yaml and values.yaml.gotmpl (templated helm values files). values.yaml references will be used verbatim. In other words: for value files ending with .gotmpl , template expressions will be rendered for plain value files (ending in .yaml ), content will be used as-is In addition to built-in ones, the following custom template functions are available: readFile reads the specified local file and generate a golang string fromYaml reads a golang string and generates a map setValueAtPath PATH NEW_VALUE traverses a golang map, replaces the value at the PATH with NEW_VALUE toYaml marshals a map into a string get returns the value of the specified key if present in the .Values object, otherwise will return the default value defined in the function Values Files Templates \u00b6 You can reference a template of values file in your helmfile.yaml like below: releases: - name: myapp chart: mychart values: - values.yaml.gotmpl Every values file whose file extension is .gotmpl is considered as a template file. Suppose values.yaml.gotmpl was something like: {{ readFile \"values.yaml\" | fromYaml | setValueAtPath \"foo.bar\" \"FOO_BAR\" | toYaml }} And values.yaml was: foo: bar: \"\" The resulting, temporary values.yaml that is generated from values.yaml.gotmpl would become: foo: # Notice `setValueAtPath \"foo.bar\" \"FOO_BAR\"` in the template above bar: FOO_BAR Refactoring helmfile.yaml with values files templates \u00b6 One of expected use-cases of values files templates is to keep helmfile.yaml small and concise. See the example helmfile.yaml below: releases: - name: {{ requiredEnv \"NAME\" }}-vault namespace: {{ requiredEnv \"NAME\" }} chart: roboll/vault-secret-manager values: - db: username: {{ requiredEnv \"DB_USERNAME\" }} password: {{ requiredEnv \"DB_PASSWORD\" }} set: - name: proxy.domain value: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com - name: proxy.scheme value: {{ env \"SCHEME\" | default \"https\" }} The values and set sections of the config file can be separated out into a template: helmfile.yaml : releases: - name: {{ requiredEnv \"NAME\" }}-vault namespace: {{ requiredEnv \"NAME\" }} chart: roboll/vault-secret-manager values: - values.yaml.gotmpl values.yaml.gotmpl : db: username: {{ requiredEnv \"DB_USERNAME\" }} password: {{ requiredEnv \"DB_PASSWORD\" }} proxy: domain: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com scheme: {{ env \"SCHEME\" | default \"https\" }} Environment \u00b6 When you want to customize the contents of helmfile.yaml or values.yaml files per environment, use this feature. You can define as many environments as you want under environments in helmfile.yaml . The environment name defaults to default , that is, helmfile sync implies the default environment. The selected environment name can be referenced from helmfile.yaml and values.yaml.gotmpl by {{ .Environment.Name }} . If you want to specify a non-default environment, provide a --environment NAME flag to helmfile like helmfile --environment production sync . The below example shows how to define a production-only release: environments: default: production: releases: - name: newrelic-agent installed: {{ eq .Environment.Name \"production\" | toYaml }} # snip - name: myapp # snip Environment Values \u00b6 Environment Values allows you to inject a set of values specific to the selected environment, into values.yaml templates. Use it to inject common values from the environment to multiple values files, to make your configuration DRY. Suppose you have three files helmfile.yaml , production.yaml and values.yaml.gotmpl : helmfile.yaml environments: production: values: - production.yaml releases: - name: myapp values: - values.yaml.gotmpl production.yaml domain: prod.example.com releaseName: prod values.yaml.gotmpl domain: {{ .Values | get \"domain\" \"dev.example.com\" }} helmfile sync installs myapp with the value domain=dev.example.com , whereas helmfile --environment production sync installs the app with the value domain=prod.example.com . For even more flexibility, you can now use values declared in the environments: section in other parts of your helmfiles: consider: default.yaml domain: dev.example.com releaseName: dev environments: default: values: - default.yaml production: values: - production.yaml # bare .yaml file, content will be used verbatim - other.yaml.gotmpl # template directives with potential side-effects like `exec` and `readFile` will be honoured releases: - name: myapp-{{ .Values.releaseName }} # release name will be one of `dev` or `prod` depending on selected environment values: - values.yaml.gotmpl - name: production-specific-release # this release would be installed only if selected environment is `production` installed: {{ eq .Values.releaseName \"prod\" | toYaml }} ... Note on Environment.Values vs Values \u00b6 The {{ .Values.foo }} syntax is the recommended way of using environment values. Prior to this pull request , environment values were made available through the {{ .Environment.Values.foo }} syntax. This is still working but is deprecated and the new {{ .Values.foo }} syntax should be used instead. You can read more infos about the feature proposal here . Loading remote environment values files \u00b6 Since #1296 and Helmfile v0.118.8, you can use go-getter -style URLs to refer to remote values files: environments: cluster-azure-us-west: values: - git::https://git.company.org/helmfiles/global/azure.yaml?ref=master - git::https://git.company.org/helmfiles/global/us-west.yaml?ref=master cluster-gcp-europe-west: values: - git::https://git.company.org/helmfiles/global/gcp.yaml?ref=master - git::https://git.company.org/helmfiles/global/europe-west.yaml?ref=master releases: - ... This is particularly useful when you co-locate helmfiles within your project repo but want to reuse the definitions in a global repo. Environment Secrets \u00b6 Environment Secrets (not to be confused with Kubernetes Secrets) are encrypted versions of Environment Values . You can list any number of secrets.yaml files created using helm secrets or sops , so that Helmfile could automatically decrypt and merge the secrets into the environment values. First you must have the helm-secrets plugin installed along with a .sops.yaml file to configure the method of encryption (this can be in the same directory as your helmfile or in the sub-directory containing your secrets files). Then suppose you have a a foo.bar secret defined in environments/production/secrets.yaml : foo.bar: \"mysupersecretstring\" You can then encrypt it with helm secrets enc environments/production/secrets.yaml Then reference that encrypted file in helmfile.yaml : environments: production: secrets: - environments/production/secrets.yaml releases: - name: myapp chart: mychart values: - values.yaml.gotmpl Then the environment secret foo.bar can be referenced by the below template expression in your values.yaml.gotmpl : {{ .Values.foo.bar }} Tillerless \u00b6 With the helm-tiller plugin installed, you can work without tiller installed. To enable this mode, you need to define tillerless: true and set the tillerNamespace in the helmDefaults section or in the releases entries. DAG-aware installation/deletion ordering \u00b6 needs controls the order of the installation/deletion of the release: releases: - name: somerelease needs: - [TILLER_NAMESPACE/][NAMESPACE/]anotherelease Be aware that you have to specify the namespace name if you configured one for the release(s). All the releases listed under needs are installed before(or deleted after) the release itself. For the following example, helmfile [sync|apply] installs releases in this order: logging servicemesh myapp1 and myapp2 - name: myapp1 chart: charts/myapp needs: - servicemesh - logging - name: myapp2 chart: charts/myapp needs: - servicemesh - logging - name: servicemesh chart: charts/istio needs: - logging - name: logging chart: charts/fluentd Note that all the releases in a same group is installed concurrently. That is, myapp1 and myapp2 are installed concurrently. On helmfile [delete|destroy] , deletions happen in the reverse order. That is, myapp1 and myapp2 are deleted first, then servicemesh , and finally logging . Separating helmfile.yaml into multiple independent files \u00b6 Once your helmfile.yaml got to contain too many releases, split it into multiple yaml files. Recommended granularity of helmfile.yaml files is \u201cper microservice\u201d or \u201cper team\u201d. And there are two ways to organize your files. Single directory Glob patterns Single directory \u00b6 helmfile -f path/to/directory loads and runs all the yaml files under the specified directory, each file as an independent helmfile.yaml. The default helmfile directory is helmfile.d , that is, in case helmfile is unable to locate helmfile.yaml , it tries to locate helmfile.d/*.yaml . All the yaml files under the specified directory are processed in the alphabetical order. For example, you can use a <two digit number>-<microservice>.yaml naming convention to control the sync order. helmfile.d / 00-database.yaml 00-backend.yaml 01-frontend.yaml Glob patterns \u00b6 In case you want more control over how multiple helmfile.yaml files are organized, use helmfiles: configuration key in the helmfile.yaml : Suppose you have multiple microservices organized in a Git repository that looks like: myteam/ (sometimes it is equivalent to a k8s ns, that is kube-system for clusterops team) apps/ filebeat/ helmfile.yaml (no charts/ exists, because it depends on the stable/filebeat chart hosted on the official helm charts repository) README.md (each app managed by my team has a dedicated README maintained by the owners of the app) metricbeat/ helmfile.yaml README.md elastalert-operator/ helmfile.yaml README.md charts/ elastalert-operator/ <the content of the local helm chart> The benefits of this structure is that you can run git diff to locate in which directory=microservice a git commit has changes. It allows your CI system to run a workflow for the changed microservice only. A downside of this is that you don\u2019t have an obvious way to sync all microservices at once. That is, you have to run: for d in apps/*; do helmfile -f $d diff; if [ $? -eq 2 ]; then helmfile -f $d sync; fi; done At this point, you\u2019ll start writing a Makefile under myteam/ so that make sync-all will do the job. It does work, but you can rely on the Helmfile feature instead. Put myteam/helmfile.yaml that looks like: helmfiles: - apps/*/helmfile.yaml So that you can get rid of the Makefile and the bash snippet. Just run helmfile sync inside myteam/ , and you are done. All the files are sorted alphabetically per group = array item inside helmfiles: , so that you have granular control over ordering, too. selectors \u00b6 When composing helmfiles you can use selectors from the command line as well as explicit selectors inside the parent helmfile to filter the releases to be used. helmfiles: - apps/*/helmfile.yaml - path: apps/a-helmfile.yaml selectors: # list of selectors - name=prometheus - tier=frontend - path: apps/b-helmfile.yaml # no selector, so all releases are used selectors: [] - path: apps/c-helmfile.yaml # parent selector to be used or cli selector for the initial helmfile selectorsInherited: true When a selector is specified, only this selector applies and the parents or CLI selectors are ignored. When not selector is specified there are 2 modes for the selector inheritance because we would like to change the current inheritance behavior (see issue #344 ). Legacy mode, sub-helmfiles without selectors inherit selectors from their parent helmfile. The initial helmfiles inherit from the command line selectors. explicit mode, sub-helmfile without selectors do not inherit from their parent or the CLI selector. If you want them to inherit from their parent selector then use selectorsInherited: true . To enable this explicit mode you need to set the following environment variable HELMFILE_EXPERIMENTAL=explicit-selector-inheritance (see experimental ). Using selector: [] will select all releases regardless of the parent selector or cli for the initial helmfile using selectorsInherited: true make the sub-helmfile selects releases with the parent selector or the cli for the initial helmfile. You cannot specify an explicit selector while using selectorsInherited: true Importing values from any source \u00b6 The exec template function that is available in values.yaml.gotmpl is useful for importing values from any source that is accessible by running a command: A usual usage of exec would look like this: mysetting: | {{ exec \"./mycmd\" (list \"arg1\" \"arg2\" \"--flag1\") | indent 2 }} Or even with a pipeline: mysetting: | {{ yourinput | exec \"./mycmd-consume-stdin\" (list \"arg1\" \"arg2\") | indent 2 }} The possibility is endless. Try importing values from your golang app, bash script, jsonnet, or anything! Hooks \u00b6 A Helmfile hook is a per-release extension point that is composed of: events command args showlogs Helmfile triggers various events while it is running. Once events are triggered, associated hooks are executed, by running the command with args . The standard output of the command will be displayed if showlogs is set and it\u2019s value is true . Currently supported events are: prepare presync preuninstall postuninstall postsync cleanup Hooks associated to prepare events are triggered after each release in your helmfile is loaded from YAML, before execution. prepare hooks are triggered on the release as long as it is not excluded by the helmfile selector(e.g. helmfile -l key=value ). Hooks associated to presync events are triggered before each release is applied to the remote cluster. This is the ideal event to execute any commands that may mutate the cluster state as it will not be run for read-only operations like lint , diff or template . preuninstall hooks are triggered immediately before a release is uninstalled as part of helmfile apply , helmfile sync , helmfile delete , and helmfile destroy . postuninstall hooks are triggered immediately after successful uninstall of a release while running helmfile apply , helmfile sync , helmfile delete , helmfile destroy . postsync hooks are triggered after each release is synced(installed, updated, or uninstalled) to/from the cluster, regardless of the sync was successful or not. This is the ideal place to execute any commands that may mutate the cluster state as it will not be run for read-only operations like lint , diff or template . cleanup hooks are triggered after each release is processed. This is the counterpart to prepare , as any release on which prepare has been triggered gets cleanup triggered as well. The following is an example hook that just prints the contextual information provided to hook: releases: - name: myapp chart: mychart # *snip* hooks: - events: [\"prepare\", \"cleanup\"] showlogs: true command: \"echo\" args: [\"{{`{{.Environment.Name}}`}}\", \"{{`{{.Release.Name}}`}}\", \"{{`{{.HelmfileCommand}}`}}\\ \"] Let\u2019s say you ran helmfile --environment prod sync , the above hook results in executing: echo {{Environment.Name}} {{.Release.Name}} {{.HelmfileCommand}} Whereas the template expressions are executed thus the command becomes: echo prod myapp sync Now, replace echo with any command you like, and rewrite args that actually conforms to the command, so that you can integrate any command that does: templating linting testing For templating, imagine that you created a hook that generates a helm chart on-the-fly by running an external tool like ksonnet, kustomize, or your own template engine. It will allow you to write your helm releases with any language you like, while still leveraging goodies provided by helm. Global Hooks \u00b6 In contrast to the per release hooks mentioned above these are run only once at the very beginning and end of the execution of a helmfile command and only the prepare and cleanup hooks are available respectively. They use the same syntax as per release hooks, but at the top level of your helmfile: hooks: - events: [\"prepare\", \"cleanup\"] showlogs: true command: \"echo\" args: [\"{{`{{.Environment.Name}}`}}\", \"{{`{{.HelmfileCommand}}`}}\\ \"] Helmfile + Kustomize \u00b6 Do you prefer kustomize to write and organize your Kubernetes apps, but still want to leverage helm\u2019s useful features like rollback, history, and so on? This section is for you! The combination of hooks and helmify-kustomize enables you to integrate kustomize into Helmfile. That is, you can use kustomize to build a local helm chart from a kustomize overlay. Let\u2019s assume you have a kustomize project named foo-kustomize like this: foo-kustomize/ \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 configMap.yaml \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 service.yaml \u2514\u2500\u2500 overlays \u251c\u2500\u2500 default \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 map.yaml \u251c\u2500\u2500 production \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 staging \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 map.yaml 5 directories, 10 files Write helmfile.yaml : - name: kustomize chart: ./foo hooks: - events: [\"prepare\", \"cleanup\"] command: \"../helmify\" args: [\"{{`{{if eq .Event.Name \\\"prepare\\\"}}build{{else}}clean{{end}}`}}\", \"{{`{{.Release.Ch\\ art}}`}}\", \"{{`{{.Environment.Name}}`}}\"] Run helmfile --environment staging sync and see it results in helmfile running kustomize build foo-kustomize/overlays/staging > foo/templates/all.yaml . Voil\u00e0! You can mix helm releases that are backed by remote charts, local charts, and even kustomize overlays. Guides \u00b6 Use the Helmfile Best Practices Guide to write advanced helmfiles that feature: Default values Layering We also have dedicated documentation on the following topics which might interest you: Shared Configurations Across Teams Or join our friendly slack community in the #helmfile channel to ask questions and get help. Check out our slack archive for good examples of how others are using it. Using .env files \u00b6 Helmfile itself doesn\u2019t have an ability to load .env files. But you can write some bash script to achieve the goal: set -a; . .env; set +a; helmfile sync Please see #203 for more context. Running Helmfile interactively \u00b6 helmfile --interactive [apply|destroy] requests confirmation from you before actually modifying your cluster. Use it when you\u2019re running helmfile manually on your local machine or a kind of secure administrative hosts. For your local use-case, aliasing it like alias hi='helmfile --interactive' would be convenient. Running Helmfile without an Internet connection \u00b6 Once you download all required charts into your machine, you can run helmfile charts to deploy your apps. It basically run only helm upgrade --install with your already-downloaded charts, hence no Internet connection is required. See #155 for more information on this topic. Experimental Features \u00b6 Some experimental features may be available for testing in perspective of being (or not) included in a future release. Those features are set using the environment variable HELMFILE_EXPERIMENTAL . Here is the current experimental feature : * explicit-selector-inheritance : remove today implicit cli selectors inheritance for composed helmfiles, see composition selector If you want to enable all experimental features set the env var to HELMFILE_EXPERIMENTAL=true bash and zsh completion \u00b6 Copy autocomplete/helmfile_bash_autocomplete or autocomplete/helmfile_zsh_autocomplete (depending on your shell of choice) to directory where you keep other shell completion scripts to make sure it is sourced. Examples \u00b6 For more examples, see the examples/README.md or the helmfile distribution by Cloud Posse . Integrations \u00b6 renovate automates chart version updates. See this PR for more information . For updating container image tags and git tags embedded within helmfile.yaml and values, you can use renovate\u2019s regexManager . Please see this comment in the renovate repository for more information. ArgoCD Integration Azure ACR Integration ArgoCD Integration \u00b6 Use ArgoCD with helmfile template for GitOps. ArgoCD has support for kustomize/manifests/helm chart by itself. Why bother with Helmfile? The reasons may vary: You do want to manage applications with ArgoCD, while letting Helmfile manage infrastructure-related components like Calico/Cilium/WeaveNet, Linkerd/Istio, and ArgoCD itself. - This way, any application deployed by ArgoCD has access to all the infrastructure. - Of course, you can use ArgoCD\u2019s Sync Waves and Phases for ordering the infrastructure and application installations. But it may be difficult to separate the concern between the infrastructure and apps and annotate K8s resources consistently when you have different teams for managing infra and apps. You want to review the exact K8s manifests being applied on pull-request time, before ArgoCD syncs. - This is often better than using a kind of HelmRelease custom resources that obfuscates exactly what manifests are being applied, which makes reviewing harder. Use Helmfile as the single-pane of glass for all the K8s resources deployed to your cluster(s). - Helmfile can reduce repetition in K8s manifests across ArgoCD application For 1, you run helmfile apply on CI to deploy ArgoCD and the infrastructure components. helmfile config for this phase often reside within the same directory as your Terraform project. So connecting the two with terraform-provider-helmfile may be helpful For 2, another app-centric CI or bot should render/commit manifests by running: helmfile template --output-dir-template $(pwd)/gitops//{{.Release.Name}} cd gitops git add . git commit -m 'some message' git push origin $BRANCH Note that $(pwd) is necessary when hemlfile.yaml has one or more sub-helmfiles in nested directories, because setting a relative file path in --output-dir or --output-dir-template results in each sub-helmfile render to the directory relative to the specified path. so that they can be deployed by Argo CD as usual. The CI or bot can optionally submit a PR to be review by human, running: hub pull-request -b main -l gitops -m 'some description' Recommendations: Do create ArgoCD Application custom resource per Helm/Helmfile release, each point to respective sub-directory generated by helmfile template --output-dir-template If you don\u2019t directly push it to the main Git branch and instead go through a pull-request, do lint rendered manifests on your CI, so that you can catch easy mistakes earlier/before ArgoCD finally deploys it See this ArgoCD issue for why you may want this, and see this helmfile issue for how --output-dir-template works. Azure ACR Integration \u00b6 Azure offers helm repository support for Azure Container Registry as a preview feature. To use this you must first az login and then az acr helm repo add -n <MyRegistry> . This will extract a token for the given ACR and configure helm to use it, e.g. helm repo update should work straight away. To use helmfile with ACR, on the other hand, you must either include a username/password in the repository definition for the ACR in your helmfile.yaml or use the --skip-deps switch, e.g. helmfile template --skip-deps . An ACR repository definition in helmfile.yaml looks like this: repositories: - name: <MyRegistry> url: https://<MyRegistry>.azurecr.io/helm/v1/repo OCI Registries \u00b6 In order to use OCI chart registries firstly they must be marked in the repository list as OCI enabled, e.g. repositories: - name: myOCIRegistry url: myregistry.azurecr.io oci: true It is important not to include a scheme for the URL as helm requires that these are not present for OCI registries Secondly the credentials for the OCI registry can either be specified within helmfile.yaml similar to repositories: - name: myOCIRegistry url: myregistry.azurecr.io oci: true username: spongebob password: squarepants or for CI scenarios these can be sourced from the environment with the format <registryName>_USERNAME and <registryName_PASSWORD> , e.g. export MYOCIREGISTRY_USERNAME=spongebob export MYOCIREGISTRY_PASSWORD=squarepants Attribution \u00b6 We use: semtag for automated semver tagging. I greatly appreciate the author(pnikosis)\u2019s effort on creating it and their kindness to share it!","title":"Home"},{"location":"#helmfile","text":"Deploy Kubernetes Helm Charts","title":"Helmfile"},{"location":"#status","text":"Even though Helmfile is used in production environments across multiple organizations , it is still in its early stage of development, hence versioned 0.x. Helmfile complies to Semantic Versioning 2.0.0 in which v0.x means that there could be backward-incompatible changes for every release. Note that we will try our best to document any backward incompatibility. And in reality, helmfile had no breaking change for a year or so.","title":"Status"},{"location":"#about","text":"Helmfile is a declarative spec for deploying helm charts. It lets you\u2026 Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm , the helmfile executable delegates to helm - as a result, helm must be installed.","title":"About"},{"location":"#highlights","text":"Declarative : Write, version-control, apply the desired state file for visibility and reproducibility. Modules : Modularize common patterns of your infrastructure, distribute it via Git, S3, etc. to be reused across the entire company (See #648 ) Versatility : Manage your cluster consisting of charts, kustomizations , and directories of Kubernetes resources, turning everything to Helm releases (See #673 ) Patch : JSON/Strategic-Merge Patch Kubernetes resources before helm-install ing, without forking upstream charts (See #673 )","title":"Highlights"},{"location":"#configuration","text":"CAUTION : This documentation is for the development version of Helmfile. If you are looking for the documentation for any of releases, please switch to the corresponding release tag like v0.92.1 . The default name for a helmfile is helmfile.yaml : # Chart repositories used from within this state file # # Use `helm-s3` and `helm-git` and whatever Helm Downloader plugins # to use repositories other than the official repository or one backend by chartmuseum. repositories: # To use official \"stable\" charts a.k.a https://github.com/helm/charts/tree/master/stable - name: stable url: https://charts.helm.sh/stable # To use official \"incubator\" charts a.k.a https://github.com/helm/charts/tree/master/incubator - name: incubator url: https://charts.helm.sh/incubator # helm-git powered repository: You can treat any Git repository as a charts repository - name: polaris url: git+https://github.com/reactiveops/polaris@deploy/helm?ref=master # Advanced configuration: You can setup basic or tls auth and optionally enable helm OCI integration - name: roboll url: http://roboll.io/charts certFile: optional_client_cert keyFile: optional_client_key username: optional_username password: optional_password oci: true # Advanced configuration: You can use a ca bundle to use an https repo # with a self-signed certificate - name: insecure url: https://charts.my-insecure-domain.com caFile: optional_ca_crt # context: kube-context # this directive is deprecated, please consider using helmDefaults.kubeContext # Path to alternative helm binary (--helm-binary) helmBinary: path/to/helm3 # Default values to set for args along with dedicated keys that can be set by contributors, cli args take precedence over these. # In other words, unset values results in no flags passed to helm. # See the helm usage (helm SUBCOMMAND -h) for more info on default values when those flags aren't provided. helmDefaults: tillerNamespace: tiller-namespace #dedicated default key for tiller-namespace tillerless: false #dedicated default key for tillerless kubeContext: kube-context #dedicated default key for kube-context (--kube-context) cleanupOnFail: false #dedicated default key for helm flag --cleanup-on-fail # additional and global args passed to helm (default \"\") args: - \"--set k=v\" # verify the chart before upgrading (only works with packaged charts not directories) (default false) verify: true # wait for k8s resources via --wait. (default false) wait: true # if set and --wait enabled, will wait until all Jobs have been completed before marking the release as successful. It will wait for as long as --timeout (default false, Implemented in Helm3.5) waitForJobs: true # time in seconds to wait for any individual Kubernetes operation (like Jobs for hooks, and waits on pod/pvc/svc/deployment readiness) (default 300) timeout: 600 # performs pods restart for the resource if applicable (default false) recreatePods: true # forces resource update through delete/recreate if needed (default false) force: false # enable TLS for request to Tiller (default false) tls: true # path to TLS CA certificate file (default \"$HELM_HOME/ca.pem\") tlsCACert: \"path/to/ca.pem\" # path to TLS certificate file (default \"$HELM_HOME/cert.pem\") tlsCert: \"path/to/cert.pem\" # path to TLS key file (default \"$HELM_HOME/key.pem\") tlsKey: \"path/to/key.pem\" # limit the maximum number of revisions saved per release. Use 0 for no limit. (default 10) historyMax: 10 # when using helm 3.2+, automatically create release namespaces if they do not exist (default true) createNamespace: true # if used with charts museum allows to pull unstable charts for deployment, for example: if 1.2.3 and 1.2.4-dev versions exist and set to true, 1.2.4-dev will be pulled (default false) devel: true # When set to `true`, skips running `helm dep up` and `helm dep build` on this release's chart. # Useful when the chart is broken, like seen in https://github.com/roboll/helmfile/issues/1547 skipDeps: false # these labels will be applied to all releases in a Helmfile. Useful in templating if you have a helmfile per environment or customer and don't want to copy the same label to each release commonLabels: hello: world # The desired states of Helm releases. # # Helmfile runs various helm commands to converge the current state in the live cluster to the desired state defined here. releases: # Published chart example - name: vault # name of this release namespace: vault # target namespace createNamespace: true # helm 3.2+ automatically create release namespace (default true) labels: # Arbitrary key value pairs for filtering releases foo: bar chart: roboll/vault-secret-manager # the chart being installed to create this release, referenced by `repository/chart` syntax version: ~1.24.1 # the semver of the chart. range constraint is supported condition: vault.enabled # The values lookup key for filtering releases. Corresponds to the boolean value of `vault.enabled`, where `vault` is an arbitrary value missingFileHandler: Warn # set to either \"Error\" or \"Warn\". \"Error\" instructs helmfile to fail when unable to find a values or secrets file. When \"Warn\", it prints the file and continues. # Values files used for rendering the chart values: # Value files passed via --values - vault.yaml # Inline values, passed via a temporary values file and --values, so that it doesn't suffer from type issues like --set - address: https://vault.example.com # Go template available in inline values and values files. - image: # The end result is more or less YAML. So do `quote` to prevent number-like strings from accidentally parsed into numbers! # See https://github.com/roboll/helmfile/issues/608 tag: {{ requiredEnv \"IMAGE_TAG\" | quote }} # Otherwise: # tag: \"{{ requiredEnv \"IMAGE_TAG\" }}\" # tag: !!string {{ requiredEnv \"IMAGE_TAG\" }} db: username: {{ requiredEnv \"DB_USERNAME\" }} # value taken from environment variable. Quotes are necessary. Will throw an error if the environment variable is not set. $DB_PASSWORD needs to be set in the calling environment ex: export DB_PASSWORD='password1' password: {{ requiredEnv \"DB_PASSWORD\" }} proxy: # Interpolate environment variable with a fixed string domain: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com scheme: {{ env \"SCHEME\" | default \"https\" }} # Use `values` whenever possible! # `set` translates to helm's `--set key=val`, that is known to suffer from type issues like https://github.com/roboll/helmfile/issues/608 set: # single value loaded from a local file, translates to --set-file foo.config=path/to/file - name: foo.config file: path/to/file # set a single array value in an array, translates to --set bar[0]={1,2} - name: bar[0] values: - 1 - 2 # set a templated value - name: namespace value: {{ .Namespace }} # will attempt to decrypt it using helm-secrets plugin secrets: - vault_secret.yaml # Override helmDefaults options for verify, wait, waitForJobs, timeout, recreatePods and force. verify: true wait: true waitForJobs: true timeout: 60 recreatePods: true force: false # set `false` to uninstall this release on sync. (default true) installed: true # restores previous state in case of failed release (default false) atomic: true # when true, cleans up any new resources created during a failed release (default false) cleanupOnFail: false # name of the tiller namespace (default \"\") tillerNamespace: vault # if true, will use the helm-tiller plugin (default false) tillerless: false # enable TLS for request to Tiller (default false) tls: true # path to TLS CA certificate file (default \"$HELM_HOME/ca.pem\") tlsCACert: \"path/to/ca.pem\" # path to TLS certificate file (default \"$HELM_HOME/cert.pem\") tlsCert: \"path/to/cert.pem\" # path to TLS key file (default \"$HELM_HOME/key.pem\") tlsKey: \"path/to/key.pem\" # --kube-context to be passed to helm commands # CAUTION: this doesn't work as expected for `tilerless: true`. # See https://github.com/roboll/helmfile/issues/642 # (default \"\", which means the standard kubeconfig, either ~/kubeconfig or the file pointed by $KUBECONFIG environment variable) kubeContext: kube-context # passes --disable-validation to helm 3 diff plugin, this requires diff plugin >= 3.1.2 # It may be helpful to deploy charts with helm api v1 CRDS # https://github.com/roboll/helmfile/pull/1373 disableValidation: false # passes --disable-validation to helm 3 diff plugin, this requires diff plugin >= 3.1.2 # It is useful when any release contains custom resources for CRDs that is not yet installed onto the cluster. # https://github.com/roboll/helmfile/pull/1618 disableValidationOnInstall: false # passes --disable-openapi-validation to helm 3 diff plugin, this requires diff plugin >= 3.1.2 # It may be helpful to deploy charts with helm api v1 CRDS # https://github.com/roboll/helmfile/pull/1373 disableOpenApiValidation: false # limit the maximum number of revisions saved per release. Use 0 for no limit (default 10) historyMax: 10 # When set to `true`, skips running `helm dep up` and `helm dep build` on this release's chart. # Useful when the chart is broken, like seen in https://github.com/roboll/helmfile/issues/1547 skipDeps: false # Local chart example - name: grafana # name of this release namespace: another # target namespace chart: ../my-charts/grafana # the chart being installed to create this release, referenced by relative path to local helmfile values: - \"../../my-values/grafana/values.yaml\" # Values file (relative path to manifest) - ./values/{{ requiredEnv \"PLATFORM_ENV\" }}/config.yaml # Values file taken from path with environment variable. $PLATFORM_ENV must be set in the calling environment. wait: true # # Advanced Configuration: Nested States # helmfiles: - # Path to the helmfile state file being processed BEFORE releases in this state file path: path/to/subhelmfile.yaml # Label selector used for filtering releases in the nested state. # For example, `name=prometheus` in this context is equivalent to processing the nested state like # helmfile -f path/to/subhelmfile.yaml -l name=prometheus sync selectors: - name=prometheus # Override state values values: # Values files merged into the nested state's values - additional.values.yaml # One important aspect of using values here is that they first need to be defined in the values section # of the origin helmfile, so in this example key1 needs to be in the values or environments.NAME.values of path/to/subhelmfile.yaml # Inline state values merged into the nested state's values - key1: val1 - # All the nested state files under `helmfiles:` is processed in the order of definition. # So it can be used for preparation for your main `releases`. An example would be creating CRDs required by `releases` in the parent state file. path: path/to/mycrd.helmfile.yaml - # Terraform-module-like URL for importing a remote directory and use a file in it as a nested-state file # The nested-state file is locally checked-out along with the remote directory containing it. # Therefore all the local paths in the file are resolved relative to the file path: git::https://github.com/cloudposse/helmfiles.git@releases/kiam.yaml?ref=0.40.0 # If set to \"Error\", return an error when a subhelmfile points to a # non-existent path. The default behavior is to print a warning and continue. missingFileHandler: Error # # Advanced Configuration: Environments # # The list of environments managed by helmfile. # # The default is `environments: {\"default\": {}}` which implies: # # - `{{ .Environment.Name }}` evaluates to \"default\" # - `{{ .Values }}` being empty environments: # The \"default\" environment is available and used when `helmfile` is run without `--environment NAME`. default: # Everything from the values.yaml is available via `{{ .Values.KEY }}`. # Suppose `{\"foo\": {\"bar\": 1}}` contained in the values.yaml below, # `{{ .Values.foo.bar }}` is evaluated to `1`. values: - environments/default/values.yaml # Each entry in values can be either a file path or inline values. # The below is an example of inline values, which is merged to the `.Values` - myChartVer: 1.0.0-dev # Any environment other than `default` is used only when `helmfile` is run with `--environment NAME`. # That is, the \"production\" env below is used when and only when it is run like `helmfile --environment production sync`. production: values: - environment/production/values.yaml - myChartVer: 1.0.0 # disable vault release processing - vault: enabled: false ## `secrets.yaml` is decrypted by `helm-secrets` and available via `{{ .Environment.Values.KEY }}` secrets: - environment/production/secrets.yaml # Instructs helmfile to fail when unable to find a environment values file listed under `environments.NAME.values`. # # Possible values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\". # # Use \"Warn\", \"Info\", or \"Debug\" if you want helmfile to not fail when a values file is missing, while just leaving # a message about the missing file at the log-level. missingFileHandler: Error # kubeContext to use for this environment kubeContext: kube-context # # Advanced Configuration: Layering # # Helmfile merges all the \"base\" state files and this state file before processing. # # Assuming this state file is named `helmfile.yaml`, all the files are merged in the order of: # environments.yaml <- defaults.yaml <- templates.yaml <- helmfile.yaml bases: - environments.yaml - defaults.yaml - templates.yaml # # Advanced Configuration: API Capabilities # # 'helmfile template' renders releases locally without querying an actual cluster, # and in this case `.Capabilities.APIVersions` cannot be populated. # When a chart queries for a specific CRD, this can lead to unexpected results. # # Configure a fixed list of api versions to pass to 'helm template' via the --api-versions flag: apiVersions: - example/v1","title":"Configuration"},{"location":"#templating","text":"Helmfile uses Go templates for templating your helmfile.yaml. While go ships several built-in functions, we have added all of the functions in the Sprig library . We also added the following functions: requiredEnv exec readFile toYaml fromYaml setValueAtPath get (Sprig\u2019s original get is available as sprigGet ) tpl required fetchSecretValue expandSecretRefs We also added one special template function: requiredEnv . The requiredEnv function allows you to declare a particular environment variable as required for template rendering. If the environment variable is unset or empty, the template rendering will fail with an error message.","title":"Templating"},{"location":"#using-environment-variables","text":"Environment variables can be used in most places for templating the helmfile. Currently this is supported for name , namespace , value (in set), values and url (in repositories). Examples: repositories: - name: your-private-git-repo-hosted-charts url: https://{{ requiredEnv \"GITHUB_TOKEN\"}}@raw.githubusercontent.com/kmzfs/helm-repo-in-github/master/ releases: - name: {{ requiredEnv \"NAME\" }}-vault namespace: {{ requiredEnv \"NAME\" }} chart: roboll/vault-secret-manager values: - db: username: {{ requiredEnv \"DB_USERNAME\" }} password: {{ requiredEnv \"DB_PASSWORD\" }} set: - name: proxy.domain value: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com - name: proxy.scheme value: {{ env \"SCHEME\" | default \"https\" }}","title":"Using environment variables"},{"location":"#note","text":"If you wish to treat your enviroment variables as strings always, even if they are boolean or numeric values you can use {{ env \"ENV_NAME\" | quote }} or \"{{ env \"ENV_NAME\" }}\" . These approaches also work with requiredEnv .","title":"Note"},{"location":"#installation","text":"download one of releases or run as a container or Archlinux: install via pacman -S helmfile or from AUR or openSUSE: install via zypper in helmfile assuming you are on Tumbleweed; if you are on Leap you must add the kubic repo for your distribution version once before that command, e.g. zypper ar https://download.opensuse.org/repositories/devel:/kubic/openSUSE_Leap_\\$releasever kubic , or Windows (using scoop ): scoop install helmfile macOS (using homebrew ): brew install helmfile","title":"Installation"},{"location":"#running-as-a-container","text":"The Helmfile Docker images are available in Quay . There is no latest tag, since the 0.x versions can contain breaking changes, so make sure you pick the right tag. Example using helmfile 0.135.0 : # helm 2 $ docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.helm:/root/.helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:v0.135.0 helmfile sync # helm 3 $ docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.config/helm:/root/.config/helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:helm3-v0.135.0 helmfile sync You can also use shims to make calling the binaries easier: # helm 2 $ printf '%s\\n' '#!/bin/sh' 'docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.helm:/root/.helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:v0.135.0 helmfile \"$@\"' | tee helmfile $ chmod +x helmfile $ ./helmfile sync # helm 3 $ printf '%s\\n' '#!/bin/sh' 'docker run --rm --net=host -v \"${HOME}/.kube:/root/.kube\" -v \"${HOME}/.config/helm:/root/.config/helm\" -v \"${PWD}:/wd\" --workdir /wd quay.io/roboll/helmfile:helm3-v0.135.0 helmfile \"$@\"' | tee helmfile $ chmod +x helmfile $ ./helmfile sync","title":"Running as a container"},{"location":"#getting-started","text":"Let\u2019s start with a simple helmfile and gradually improve it to fit your use-case! Suppose the helmfile.yaml representing the desired state of your helm releases looks like: releases: - name: prom-norbac-ubuntu namespace: prometheus chart: stable/prometheus set: - name: rbac.create value: false Sync your Kubernetes cluster state to the desired one by running: helmfile apply Congratulations! You now have your first Prometheus deployment running inside your cluster. Iterate on the helmfile.yaml by referencing: Configuration CLI reference . Helmfile Best Practices Guide","title":"Getting Started"},{"location":"#cli-reference","text":"NAME: helmfile USAGE: helmfile [global options] command [command options] [arguments...] VERSION: v0.138.6 COMMANDS: deps update charts based on their requirements repos sync repositories from state file (helm repo add && helm repo update) charts DEPRECATED: sync releases from state file (helm upgrade --install) diff diff releases from state file against env (helm diff) template template releases from state file against env (helm template) write-values write values files for releases. Similar to `helmfile template`, write values files instead of manifests. lint lint charts from state file (helm lint) sync sync all resources from state file (repos, releases and chart deps) apply apply all resources from state file only when there are changes status retrieve status of releases in state file delete DEPRECATED: delete releases from state file (helm delete) destroy deletes and then purges releases test test releases from state file (helm test) build output compiled helmfile state(s) as YAML list list releases defined in state file fetch fetch charts from state file version Show the version for Helmfile. help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --helm-binary value, -b value path to helm binary (default: \"helm\") --file helmfile.yaml, -f helmfile.yaml load config from file or directory. defaults to helmfile.yaml or `helmfile.d`(means `helmfile.d/*.yaml`) in this preference --environment value, -e value specify the environment name. defaults to \"default\" --state-values-set value set state values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --state-values-file value specify state values in a YAML file --quiet, -q Silence output. Equivalent to log-level warn --kube-context value Set kubectl context. Uses current context by default --debug Enable verbose output for Helm and set log-level to debug, this disables --quiet/-q effect --no-color Output without color --log-level value Set log level, default info --namespace value, -n value Set namespace. Uses the namespace set in the context by default, and is available in templates as {{ .Namespace }} --selector value, -l value Only run using the releases that match labels. Labels can take the form of foo=bar or foo!=bar. A release must match all labels in a group in order to be used. Multiple groups can be specified at once. --selector tier=frontend,tier!=proxy --selector tier=backend. Will match all frontend, non-proxy releases AND all backend releases. The name of a release can be used as a label. --selector name=myrelease --allow-no-matching-release Do not exit with an error code if the provided selector has no matching releases. --interactive, -i Request confirmation before attempting to modify clusters --help, -h show help --version, -v print the version","title":"CLI Reference"},{"location":"#sync","text":"The helmfile sync sub-command sync your cluster state as described in your helmfile . The default helmfile is helmfile.yaml , but any YAML file can be passed by specifying a --file path/to/your/yaml/file flag. Under the covers, Helmfile executes helm upgrade --install for each release declared in the manifest, by optionally decrypting secrets to be consumed as helm chart values. It also updates specified chart repositories and updates the dependencies of any referenced local charts. For Helm 2.9+ you can use a username and password to authenticate to a remote repository.","title":"sync"},{"location":"#deps","text":"The helmfile deps sub-command locks your helmfile state and local charts dependencies. It basically runs helm dependency update on your helmfile state file and all the referenced local charts, so that you get a \u201clock\u201d file per each helmfile state or local chart. All the other helmfile sub-commands like sync use chart versions recorded in the lock files, so that e.g. untested chart versions won\u2019t suddenly get deployed to the production environment. For example, the lock file for a helmfile state file named helmfile.1.yaml will be helmfile.1.lock . The lock file for a local chart would be requirements.lock , which is the same as helm . It is recommended to version-control all the lock files, so that they can be used in the production deployment pipeline for extra reproducibility. To bring in chart updates systematically, it would also be a good idea to run helmfile deps regularly, test it, and then update the lock files in the version-control system.","title":"deps"},{"location":"#diff","text":"The helmfile diff sub-command executes the helm-diff plugin across all of the charts/releases defined in the manifest. To supply the diff functionality Helmfile needs the helm-diff plugin v2.9.0+1 or greater installed. For Helm 2.3+ you should be able to simply execute helm plugin install https://github.com/databus23/helm-diff . For more details please look at their documentation .","title":"diff"},{"location":"#apply","text":"The helmfile apply sub-command begins by executing diff . If diff finds that there is any changes, sync is executed. Adding --interactive instructs Helmfile to request your confirmation before sync . An expected use-case of apply is to schedule it to run periodically, so that you can auto-fix skews between the desired and the current state of your apps running on Kubernetes clusters.","title":"apply"},{"location":"#destroy","text":"The helmfile destroy sub-command uninstalls and purges all the releases defined in the manifests. helmfile --interactive destroy instructs Helmfile to request your confirmation before actually deleting releases. destroy basically runs helm uninstall --purge on all the targeted releases. If you don\u2019t want purging, use helmfile delete instead.","title":"destroy"},{"location":"#delete-deprecated","text":"The helmfile delete sub-command deletes all the releases defined in the manifests. helmfile --interactive delete instructs Helmfile to request your confirmation before actually deleting releases. Note that delete doesn\u2019t purge releases. So helmfile delete && helmfile sync results in sync failed due to that releases names are not deleted but preserved for future references. If you really want to remove releases for reuse, add --purge flag to run it like helmfile delete --purge .","title":"delete (DEPRECATED)"},{"location":"#secrets","text":"The secrets parameter in a helmfile.yaml causes the helm-secrets plugin to be executed to decrypt the file. To supply the secret functionality Helmfile needs the helm secrets plugin installed. For Helm 2.3+ you should be able to simply execute helm plugin install https://github.com/jkroepke/helm-secrets .","title":"secrets"},{"location":"#test","text":"The helmfile test sub-command runs a helm test against specified releases in the manifest, default to all Use --cleanup to delete pods upon completion.","title":"test"},{"location":"#lint","text":"The helmfile lint sub-command runs a helm lint across all of the charts/releases defined in the manifest. Non local charts will be fetched into a temporary folder which will be deleted once the task is completed.","title":"lint"},{"location":"#fetch","text":"The helmfile fetch sub-command downloads or copies local charts to a local directory for debug purpose. The local directory must be specified with --output-dir .","title":"fetch"},{"location":"#paths-overview","text":"Using manifest files in conjunction with command line argument can be a bit confusing. A few rules to clear up this ambiguity: Absolute paths are always resolved as absolute paths Relative paths referenced in the Helmfile manifest itself are relative to that manifest Relative paths referenced on the command line are relative to the current working directory the user is in For additional context, take a look at paths examples .","title":"Paths Overview"},{"location":"#labels-overview","text":"A selector can be used to only target a subset of releases when running Helmfile. This is useful for large helmfiles with releases that are logically grouped together. Labels are simple key value pairs that are an optional field of the release spec. When selecting by label, the search can be inverted. tier!=backend would match all releases that do NOT have the tier: backend label. tier=fronted would only match releases with the tier: frontend label. Multiple labels can be specified using , as a separator. A release must match all selectors in order to be selected for the final helm command. The selector parameter can be specified multiple times. Each parameter is resolved independently so a release that matches any parameter will be used. --selector tier=frontend --selector tier=backend will select all the charts. In addition to user supplied labels, the name, the namespace, and the chart are available to be used as selectors. The chart will just be the chart name excluding the repository (Example stable/filebeat would be selected using --selector chart=filebeat ). commonLabels can be used when you want to apply the same label to all releases and use templating based on that. For instance, you install a number of charts on every customer but need to provide different values file per customer. templates/common.yaml: templates: nginx: &nginx name: nginx chart: stable/nginx-ingress values: - ../values/common/{{ .Release.Name }}.yaml - ../values/{{ .Release.Labels.customer }}/{{ .Release.Name }}.yaml cert-manager: &cert-manager name: cert-manager chart: jetstack/cert-manager values: - ../values/common/{{ .Release.Name }}.yaml - ../values/{{ .Release.Labels.customer }}/{{ .Release.Name }}.yaml helmfile.yaml: {{ readFile \"templates/common.yaml\" }} commonLabels: customer: company releases: - <<: *nginx - <<: *cert-manager","title":"Labels Overview"},{"location":"#templates","text":"You can use go\u2019s text/template expressions in helmfile.yaml and values.yaml.gotmpl (templated helm values files). values.yaml references will be used verbatim. In other words: for value files ending with .gotmpl , template expressions will be rendered for plain value files (ending in .yaml ), content will be used as-is In addition to built-in ones, the following custom template functions are available: readFile reads the specified local file and generate a golang string fromYaml reads a golang string and generates a map setValueAtPath PATH NEW_VALUE traverses a golang map, replaces the value at the PATH with NEW_VALUE toYaml marshals a map into a string get returns the value of the specified key if present in the .Values object, otherwise will return the default value defined in the function","title":"Templates"},{"location":"#values-files-templates","text":"You can reference a template of values file in your helmfile.yaml like below: releases: - name: myapp chart: mychart values: - values.yaml.gotmpl Every values file whose file extension is .gotmpl is considered as a template file. Suppose values.yaml.gotmpl was something like: {{ readFile \"values.yaml\" | fromYaml | setValueAtPath \"foo.bar\" \"FOO_BAR\" | toYaml }} And values.yaml was: foo: bar: \"\" The resulting, temporary values.yaml that is generated from values.yaml.gotmpl would become: foo: # Notice `setValueAtPath \"foo.bar\" \"FOO_BAR\"` in the template above bar: FOO_BAR","title":"Values Files Templates"},{"location":"#refactoring-helmfileyaml-with-values-files-templates","text":"One of expected use-cases of values files templates is to keep helmfile.yaml small and concise. See the example helmfile.yaml below: releases: - name: {{ requiredEnv \"NAME\" }}-vault namespace: {{ requiredEnv \"NAME\" }} chart: roboll/vault-secret-manager values: - db: username: {{ requiredEnv \"DB_USERNAME\" }} password: {{ requiredEnv \"DB_PASSWORD\" }} set: - name: proxy.domain value: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com - name: proxy.scheme value: {{ env \"SCHEME\" | default \"https\" }} The values and set sections of the config file can be separated out into a template: helmfile.yaml : releases: - name: {{ requiredEnv \"NAME\" }}-vault namespace: {{ requiredEnv \"NAME\" }} chart: roboll/vault-secret-manager values: - values.yaml.gotmpl values.yaml.gotmpl : db: username: {{ requiredEnv \"DB_USERNAME\" }} password: {{ requiredEnv \"DB_PASSWORD\" }} proxy: domain: {{ requiredEnv \"PLATFORM_ID\" }}.my-domain.com scheme: {{ env \"SCHEME\" | default \"https\" }}","title":"Refactoring helmfile.yaml with values files templates"},{"location":"#environment","text":"When you want to customize the contents of helmfile.yaml or values.yaml files per environment, use this feature. You can define as many environments as you want under environments in helmfile.yaml . The environment name defaults to default , that is, helmfile sync implies the default environment. The selected environment name can be referenced from helmfile.yaml and values.yaml.gotmpl by {{ .Environment.Name }} . If you want to specify a non-default environment, provide a --environment NAME flag to helmfile like helmfile --environment production sync . The below example shows how to define a production-only release: environments: default: production: releases: - name: newrelic-agent installed: {{ eq .Environment.Name \"production\" | toYaml }} # snip - name: myapp # snip","title":"Environment"},{"location":"#environment-values","text":"Environment Values allows you to inject a set of values specific to the selected environment, into values.yaml templates. Use it to inject common values from the environment to multiple values files, to make your configuration DRY. Suppose you have three files helmfile.yaml , production.yaml and values.yaml.gotmpl : helmfile.yaml environments: production: values: - production.yaml releases: - name: myapp values: - values.yaml.gotmpl production.yaml domain: prod.example.com releaseName: prod values.yaml.gotmpl domain: {{ .Values | get \"domain\" \"dev.example.com\" }} helmfile sync installs myapp with the value domain=dev.example.com , whereas helmfile --environment production sync installs the app with the value domain=prod.example.com . For even more flexibility, you can now use values declared in the environments: section in other parts of your helmfiles: consider: default.yaml domain: dev.example.com releaseName: dev environments: default: values: - default.yaml production: values: - production.yaml # bare .yaml file, content will be used verbatim - other.yaml.gotmpl # template directives with potential side-effects like `exec` and `readFile` will be honoured releases: - name: myapp-{{ .Values.releaseName }} # release name will be one of `dev` or `prod` depending on selected environment values: - values.yaml.gotmpl - name: production-specific-release # this release would be installed only if selected environment is `production` installed: {{ eq .Values.releaseName \"prod\" | toYaml }} ...","title":"Environment Values"},{"location":"#note-on-environmentvalues-vs-values","text":"The {{ .Values.foo }} syntax is the recommended way of using environment values. Prior to this pull request , environment values were made available through the {{ .Environment.Values.foo }} syntax. This is still working but is deprecated and the new {{ .Values.foo }} syntax should be used instead. You can read more infos about the feature proposal here .","title":"Note on Environment.Values vs Values"},{"location":"#loading-remote-environment-values-files","text":"Since #1296 and Helmfile v0.118.8, you can use go-getter -style URLs to refer to remote values files: environments: cluster-azure-us-west: values: - git::https://git.company.org/helmfiles/global/azure.yaml?ref=master - git::https://git.company.org/helmfiles/global/us-west.yaml?ref=master cluster-gcp-europe-west: values: - git::https://git.company.org/helmfiles/global/gcp.yaml?ref=master - git::https://git.company.org/helmfiles/global/europe-west.yaml?ref=master releases: - ... This is particularly useful when you co-locate helmfiles within your project repo but want to reuse the definitions in a global repo.","title":"Loading remote environment values files"},{"location":"#environment-secrets","text":"Environment Secrets (not to be confused with Kubernetes Secrets) are encrypted versions of Environment Values . You can list any number of secrets.yaml files created using helm secrets or sops , so that Helmfile could automatically decrypt and merge the secrets into the environment values. First you must have the helm-secrets plugin installed along with a .sops.yaml file to configure the method of encryption (this can be in the same directory as your helmfile or in the sub-directory containing your secrets files). Then suppose you have a a foo.bar secret defined in environments/production/secrets.yaml : foo.bar: \"mysupersecretstring\" You can then encrypt it with helm secrets enc environments/production/secrets.yaml Then reference that encrypted file in helmfile.yaml : environments: production: secrets: - environments/production/secrets.yaml releases: - name: myapp chart: mychart values: - values.yaml.gotmpl Then the environment secret foo.bar can be referenced by the below template expression in your values.yaml.gotmpl : {{ .Values.foo.bar }}","title":"Environment Secrets"},{"location":"#tillerless","text":"With the helm-tiller plugin installed, you can work without tiller installed. To enable this mode, you need to define tillerless: true and set the tillerNamespace in the helmDefaults section or in the releases entries.","title":"Tillerless"},{"location":"#dag-aware-installationdeletion-ordering","text":"needs controls the order of the installation/deletion of the release: releases: - name: somerelease needs: - [TILLER_NAMESPACE/][NAMESPACE/]anotherelease Be aware that you have to specify the namespace name if you configured one for the release(s). All the releases listed under needs are installed before(or deleted after) the release itself. For the following example, helmfile [sync|apply] installs releases in this order: logging servicemesh myapp1 and myapp2 - name: myapp1 chart: charts/myapp needs: - servicemesh - logging - name: myapp2 chart: charts/myapp needs: - servicemesh - logging - name: servicemesh chart: charts/istio needs: - logging - name: logging chart: charts/fluentd Note that all the releases in a same group is installed concurrently. That is, myapp1 and myapp2 are installed concurrently. On helmfile [delete|destroy] , deletions happen in the reverse order. That is, myapp1 and myapp2 are deleted first, then servicemesh , and finally logging .","title":"DAG-aware installation/deletion ordering"},{"location":"#separating-helmfileyaml-into-multiple-independent-files","text":"Once your helmfile.yaml got to contain too many releases, split it into multiple yaml files. Recommended granularity of helmfile.yaml files is \u201cper microservice\u201d or \u201cper team\u201d. And there are two ways to organize your files. Single directory Glob patterns","title":"Separating helmfile.yaml into multiple independent files"},{"location":"#single-directory","text":"helmfile -f path/to/directory loads and runs all the yaml files under the specified directory, each file as an independent helmfile.yaml. The default helmfile directory is helmfile.d , that is, in case helmfile is unable to locate helmfile.yaml , it tries to locate helmfile.d/*.yaml . All the yaml files under the specified directory are processed in the alphabetical order. For example, you can use a <two digit number>-<microservice>.yaml naming convention to control the sync order. helmfile.d / 00-database.yaml 00-backend.yaml 01-frontend.yaml","title":"Single directory"},{"location":"#glob-patterns","text":"In case you want more control over how multiple helmfile.yaml files are organized, use helmfiles: configuration key in the helmfile.yaml : Suppose you have multiple microservices organized in a Git repository that looks like: myteam/ (sometimes it is equivalent to a k8s ns, that is kube-system for clusterops team) apps/ filebeat/ helmfile.yaml (no charts/ exists, because it depends on the stable/filebeat chart hosted on the official helm charts repository) README.md (each app managed by my team has a dedicated README maintained by the owners of the app) metricbeat/ helmfile.yaml README.md elastalert-operator/ helmfile.yaml README.md charts/ elastalert-operator/ <the content of the local helm chart> The benefits of this structure is that you can run git diff to locate in which directory=microservice a git commit has changes. It allows your CI system to run a workflow for the changed microservice only. A downside of this is that you don\u2019t have an obvious way to sync all microservices at once. That is, you have to run: for d in apps/*; do helmfile -f $d diff; if [ $? -eq 2 ]; then helmfile -f $d sync; fi; done At this point, you\u2019ll start writing a Makefile under myteam/ so that make sync-all will do the job. It does work, but you can rely on the Helmfile feature instead. Put myteam/helmfile.yaml that looks like: helmfiles: - apps/*/helmfile.yaml So that you can get rid of the Makefile and the bash snippet. Just run helmfile sync inside myteam/ , and you are done. All the files are sorted alphabetically per group = array item inside helmfiles: , so that you have granular control over ordering, too.","title":"Glob patterns"},{"location":"#selectors","text":"When composing helmfiles you can use selectors from the command line as well as explicit selectors inside the parent helmfile to filter the releases to be used. helmfiles: - apps/*/helmfile.yaml - path: apps/a-helmfile.yaml selectors: # list of selectors - name=prometheus - tier=frontend - path: apps/b-helmfile.yaml # no selector, so all releases are used selectors: [] - path: apps/c-helmfile.yaml # parent selector to be used or cli selector for the initial helmfile selectorsInherited: true When a selector is specified, only this selector applies and the parents or CLI selectors are ignored. When not selector is specified there are 2 modes for the selector inheritance because we would like to change the current inheritance behavior (see issue #344 ). Legacy mode, sub-helmfiles without selectors inherit selectors from their parent helmfile. The initial helmfiles inherit from the command line selectors. explicit mode, sub-helmfile without selectors do not inherit from their parent or the CLI selector. If you want them to inherit from their parent selector then use selectorsInherited: true . To enable this explicit mode you need to set the following environment variable HELMFILE_EXPERIMENTAL=explicit-selector-inheritance (see experimental ). Using selector: [] will select all releases regardless of the parent selector or cli for the initial helmfile using selectorsInherited: true make the sub-helmfile selects releases with the parent selector or the cli for the initial helmfile. You cannot specify an explicit selector while using selectorsInherited: true","title":"selectors"},{"location":"#importing-values-from-any-source","text":"The exec template function that is available in values.yaml.gotmpl is useful for importing values from any source that is accessible by running a command: A usual usage of exec would look like this: mysetting: | {{ exec \"./mycmd\" (list \"arg1\" \"arg2\" \"--flag1\") | indent 2 }} Or even with a pipeline: mysetting: | {{ yourinput | exec \"./mycmd-consume-stdin\" (list \"arg1\" \"arg2\") | indent 2 }} The possibility is endless. Try importing values from your golang app, bash script, jsonnet, or anything!","title":"Importing values from any source"},{"location":"#hooks","text":"A Helmfile hook is a per-release extension point that is composed of: events command args showlogs Helmfile triggers various events while it is running. Once events are triggered, associated hooks are executed, by running the command with args . The standard output of the command will be displayed if showlogs is set and it\u2019s value is true . Currently supported events are: prepare presync preuninstall postuninstall postsync cleanup Hooks associated to prepare events are triggered after each release in your helmfile is loaded from YAML, before execution. prepare hooks are triggered on the release as long as it is not excluded by the helmfile selector(e.g. helmfile -l key=value ). Hooks associated to presync events are triggered before each release is applied to the remote cluster. This is the ideal event to execute any commands that may mutate the cluster state as it will not be run for read-only operations like lint , diff or template . preuninstall hooks are triggered immediately before a release is uninstalled as part of helmfile apply , helmfile sync , helmfile delete , and helmfile destroy . postuninstall hooks are triggered immediately after successful uninstall of a release while running helmfile apply , helmfile sync , helmfile delete , helmfile destroy . postsync hooks are triggered after each release is synced(installed, updated, or uninstalled) to/from the cluster, regardless of the sync was successful or not. This is the ideal place to execute any commands that may mutate the cluster state as it will not be run for read-only operations like lint , diff or template . cleanup hooks are triggered after each release is processed. This is the counterpart to prepare , as any release on which prepare has been triggered gets cleanup triggered as well. The following is an example hook that just prints the contextual information provided to hook: releases: - name: myapp chart: mychart # *snip* hooks: - events: [\"prepare\", \"cleanup\"] showlogs: true command: \"echo\" args: [\"{{`{{.Environment.Name}}`}}\", \"{{`{{.Release.Name}}`}}\", \"{{`{{.HelmfileCommand}}`}}\\ \"] Let\u2019s say you ran helmfile --environment prod sync , the above hook results in executing: echo {{Environment.Name}} {{.Release.Name}} {{.HelmfileCommand}} Whereas the template expressions are executed thus the command becomes: echo prod myapp sync Now, replace echo with any command you like, and rewrite args that actually conforms to the command, so that you can integrate any command that does: templating linting testing For templating, imagine that you created a hook that generates a helm chart on-the-fly by running an external tool like ksonnet, kustomize, or your own template engine. It will allow you to write your helm releases with any language you like, while still leveraging goodies provided by helm.","title":"Hooks"},{"location":"#global-hooks","text":"In contrast to the per release hooks mentioned above these are run only once at the very beginning and end of the execution of a helmfile command and only the prepare and cleanup hooks are available respectively. They use the same syntax as per release hooks, but at the top level of your helmfile: hooks: - events: [\"prepare\", \"cleanup\"] showlogs: true command: \"echo\" args: [\"{{`{{.Environment.Name}}`}}\", \"{{`{{.HelmfileCommand}}`}}\\ \"]","title":"Global Hooks"},{"location":"#helmfile-kustomize","text":"Do you prefer kustomize to write and organize your Kubernetes apps, but still want to leverage helm\u2019s useful features like rollback, history, and so on? This section is for you! The combination of hooks and helmify-kustomize enables you to integrate kustomize into Helmfile. That is, you can use kustomize to build a local helm chart from a kustomize overlay. Let\u2019s assume you have a kustomize project named foo-kustomize like this: foo-kustomize/ \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 configMap.yaml \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 service.yaml \u2514\u2500\u2500 overlays \u251c\u2500\u2500 default \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 map.yaml \u251c\u2500\u2500 production \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 staging \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 map.yaml 5 directories, 10 files Write helmfile.yaml : - name: kustomize chart: ./foo hooks: - events: [\"prepare\", \"cleanup\"] command: \"../helmify\" args: [\"{{`{{if eq .Event.Name \\\"prepare\\\"}}build{{else}}clean{{end}}`}}\", \"{{`{{.Release.Ch\\ art}}`}}\", \"{{`{{.Environment.Name}}`}}\"] Run helmfile --environment staging sync and see it results in helmfile running kustomize build foo-kustomize/overlays/staging > foo/templates/all.yaml . Voil\u00e0! You can mix helm releases that are backed by remote charts, local charts, and even kustomize overlays.","title":"Helmfile + Kustomize"},{"location":"#guides","text":"Use the Helmfile Best Practices Guide to write advanced helmfiles that feature: Default values Layering We also have dedicated documentation on the following topics which might interest you: Shared Configurations Across Teams Or join our friendly slack community in the #helmfile channel to ask questions and get help. Check out our slack archive for good examples of how others are using it.","title":"Guides"},{"location":"#using-env-files","text":"Helmfile itself doesn\u2019t have an ability to load .env files. But you can write some bash script to achieve the goal: set -a; . .env; set +a; helmfile sync Please see #203 for more context.","title":"Using .env files"},{"location":"#running-helmfile-interactively","text":"helmfile --interactive [apply|destroy] requests confirmation from you before actually modifying your cluster. Use it when you\u2019re running helmfile manually on your local machine or a kind of secure administrative hosts. For your local use-case, aliasing it like alias hi='helmfile --interactive' would be convenient.","title":"Running Helmfile interactively"},{"location":"#running-helmfile-without-an-internet-connection","text":"Once you download all required charts into your machine, you can run helmfile charts to deploy your apps. It basically run only helm upgrade --install with your already-downloaded charts, hence no Internet connection is required. See #155 for more information on this topic.","title":"Running Helmfile without an Internet connection"},{"location":"#experimental-features","text":"Some experimental features may be available for testing in perspective of being (or not) included in a future release. Those features are set using the environment variable HELMFILE_EXPERIMENTAL . Here is the current experimental feature : * explicit-selector-inheritance : remove today implicit cli selectors inheritance for composed helmfiles, see composition selector If you want to enable all experimental features set the env var to HELMFILE_EXPERIMENTAL=true","title":"Experimental Features"},{"location":"#bash-and-zsh-completion","text":"Copy autocomplete/helmfile_bash_autocomplete or autocomplete/helmfile_zsh_autocomplete (depending on your shell of choice) to directory where you keep other shell completion scripts to make sure it is sourced.","title":"bash and zsh completion"},{"location":"#examples","text":"For more examples, see the examples/README.md or the helmfile distribution by Cloud Posse .","title":"Examples"},{"location":"#integrations","text":"renovate automates chart version updates. See this PR for more information . For updating container image tags and git tags embedded within helmfile.yaml and values, you can use renovate\u2019s regexManager . Please see this comment in the renovate repository for more information. ArgoCD Integration Azure ACR Integration","title":"Integrations"},{"location":"#argocd-integration","text":"Use ArgoCD with helmfile template for GitOps. ArgoCD has support for kustomize/manifests/helm chart by itself. Why bother with Helmfile? The reasons may vary: You do want to manage applications with ArgoCD, while letting Helmfile manage infrastructure-related components like Calico/Cilium/WeaveNet, Linkerd/Istio, and ArgoCD itself. - This way, any application deployed by ArgoCD has access to all the infrastructure. - Of course, you can use ArgoCD\u2019s Sync Waves and Phases for ordering the infrastructure and application installations. But it may be difficult to separate the concern between the infrastructure and apps and annotate K8s resources consistently when you have different teams for managing infra and apps. You want to review the exact K8s manifests being applied on pull-request time, before ArgoCD syncs. - This is often better than using a kind of HelmRelease custom resources that obfuscates exactly what manifests are being applied, which makes reviewing harder. Use Helmfile as the single-pane of glass for all the K8s resources deployed to your cluster(s). - Helmfile can reduce repetition in K8s manifests across ArgoCD application For 1, you run helmfile apply on CI to deploy ArgoCD and the infrastructure components. helmfile config for this phase often reside within the same directory as your Terraform project. So connecting the two with terraform-provider-helmfile may be helpful For 2, another app-centric CI or bot should render/commit manifests by running: helmfile template --output-dir-template $(pwd)/gitops//{{.Release.Name}} cd gitops git add . git commit -m 'some message' git push origin $BRANCH Note that $(pwd) is necessary when hemlfile.yaml has one or more sub-helmfiles in nested directories, because setting a relative file path in --output-dir or --output-dir-template results in each sub-helmfile render to the directory relative to the specified path. so that they can be deployed by Argo CD as usual. The CI or bot can optionally submit a PR to be review by human, running: hub pull-request -b main -l gitops -m 'some description' Recommendations: Do create ArgoCD Application custom resource per Helm/Helmfile release, each point to respective sub-directory generated by helmfile template --output-dir-template If you don\u2019t directly push it to the main Git branch and instead go through a pull-request, do lint rendered manifests on your CI, so that you can catch easy mistakes earlier/before ArgoCD finally deploys it See this ArgoCD issue for why you may want this, and see this helmfile issue for how --output-dir-template works.","title":"ArgoCD Integration"},{"location":"#azure-acr-integration","text":"Azure offers helm repository support for Azure Container Registry as a preview feature. To use this you must first az login and then az acr helm repo add -n <MyRegistry> . This will extract a token for the given ACR and configure helm to use it, e.g. helm repo update should work straight away. To use helmfile with ACR, on the other hand, you must either include a username/password in the repository definition for the ACR in your helmfile.yaml or use the --skip-deps switch, e.g. helmfile template --skip-deps . An ACR repository definition in helmfile.yaml looks like this: repositories: - name: <MyRegistry> url: https://<MyRegistry>.azurecr.io/helm/v1/repo","title":"Azure ACR Integration"},{"location":"#oci-registries","text":"In order to use OCI chart registries firstly they must be marked in the repository list as OCI enabled, e.g. repositories: - name: myOCIRegistry url: myregistry.azurecr.io oci: true It is important not to include a scheme for the URL as helm requires that these are not present for OCI registries Secondly the credentials for the OCI registry can either be specified within helmfile.yaml similar to repositories: - name: myOCIRegistry url: myregistry.azurecr.io oci: true username: spongebob password: squarepants or for CI scenarios these can be sourced from the environment with the format <registryName>_USERNAME and <registryName_PASSWORD> , e.g. export MYOCIREGISTRY_USERNAME=spongebob export MYOCIREGISTRY_PASSWORD=squarepants","title":"OCI Registries"},{"location":"#attribution","text":"We use: semtag for automated semver tagging. I greatly appreciate the author(pnikosis)\u2019s effort on creating it and their kindness to share it!","title":"Attribution"},{"location":"CONTRIBUTING/","text":"DCO + License \u00b6 By contributing to helmfile , you accept and agree to the following DCO and license terms and conditions for your present and future Contributions submitted to the helmfile project. DCO License Developing helmfile \u00b6 Locate your GOPATH , usually ~/go , and run: $ go get github.com/roboll/helmfile $ cd $GOPATH/src/github.com/roboll/helmfile $ git checkout -b your-shiny-new-feature origin/master ... $ git commit -m 'feat: do whatever for whatever purpose This adds ... by: - Adding ... - Changing ... - Removing... Resolves #ISSUE_NUMBER ' $ hub fork $ git push YOUR_GITHUB_USER your-shiny-new-feature $ hub pull-request Note that the above tutorial uses hub just for ease of explanation. Please use whatever tool or way to author your pull request!","title":"Contributing"},{"location":"CONTRIBUTING/#dco-license","text":"By contributing to helmfile , you accept and agree to the following DCO and license terms and conditions for your present and future Contributions submitted to the helmfile project. DCO License","title":"DCO + License"},{"location":"CONTRIBUTING/#developing-helmfile","text":"Locate your GOPATH , usually ~/go , and run: $ go get github.com/roboll/helmfile $ cd $GOPATH/src/github.com/roboll/helmfile $ git checkout -b your-shiny-new-feature origin/master ... $ git commit -m 'feat: do whatever for whatever purpose This adds ... by: - Adding ... - Changing ... - Removing... Resolves #ISSUE_NUMBER ' $ hub fork $ git push YOUR_GITHUB_USER your-shiny-new-feature $ hub pull-request Note that the above tutorial uses hub just for ease of explanation. Please use whatever tool or way to author your pull request!","title":"Developing helmfile"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2017 rob boll Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"PATHS/","text":"Paths Overivew \u00b6 Using manifest files in conjunction with command line argument can be a bit confusing. A few rules to clear up this ambiguity: Absolute paths are always resolved as absolute paths Relative paths referenced in the helmfile manifest itself are relative to that manifest Relative paths referenced on the command line are relative to the current working directory the user is in Examples \u00b6 There are several examples that we can go through in the /examples folder which demonstrate this. Local Execution This is an example of a Helmfile manifest referencing a local value directly. Indirect: helmfile -f examples/deployments/local/charts.yaml sync Direct: cd examples/deployments/local/ helmfile sync Relative Paths in Helmfile This is an example of a Helmfile manifest using relative paths for values. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync Direct: cd examples/deployments/dev/ helmfile sync Relative Paths in Helmfile w/ \u2013values overrides This is an example of a Helmfile manifest using relative paths for values including an additional --values from the command line. NOTE: The --values is resolved relative to the CWD of the terminal not the Helmfile manifest. You can see this with the replicas being adjusted to 3 now for the deployment. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync --values values/replica-values.yaml Direct: cd examples/deployments/dev/ helmfile sync --values ../../values/replica-values.yaml","title":"Paths Overview"},{"location":"PATHS/#paths-overivew","text":"Using manifest files in conjunction with command line argument can be a bit confusing. A few rules to clear up this ambiguity: Absolute paths are always resolved as absolute paths Relative paths referenced in the helmfile manifest itself are relative to that manifest Relative paths referenced on the command line are relative to the current working directory the user is in","title":"Paths Overivew"},{"location":"PATHS/#examples","text":"There are several examples that we can go through in the /examples folder which demonstrate this. Local Execution This is an example of a Helmfile manifest referencing a local value directly. Indirect: helmfile -f examples/deployments/local/charts.yaml sync Direct: cd examples/deployments/local/ helmfile sync Relative Paths in Helmfile This is an example of a Helmfile manifest using relative paths for values. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync Direct: cd examples/deployments/dev/ helmfile sync Relative Paths in Helmfile w/ \u2013values overrides This is an example of a Helmfile manifest using relative paths for values including an additional --values from the command line. NOTE: The --values is resolved relative to the CWD of the terminal not the Helmfile manifest. You can see this with the replicas being adjusted to 3 now for the deployment. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync --values values/replica-values.yaml Direct: cd examples/deployments/dev/ helmfile sync --values ../../values/replica-values.yaml","title":"Examples"},{"location":"USERS/","text":"Helmfile Users \u00b6 Who\u2019s actually using Helmfile? The following table is compiled from public information, survey response and PRs. It\u2019s likely this is only a small selection of actual users, but it\u2019s a start. If you are using Helmfile in your company or organization, we would like to invite you to create a PR to add your information to this file. Organization Workload More Info Location Added reddit.com production GitHub issue , Talk San Francisco, CA April 2018 ricardo.ch production We\u2019re deploying our complete application platform using Helmfile. Zug, Switzerland April 2018 Hellofresh production We\u2019ve been using helmfile in production since April 2018 for deploying all our infrastructure applications Berlin, Germany April 2018 Cherre production We have no public posts about using Helmfile, but we have been using it for a long time now New York, NY October 2018 Sight Machine production We don\u2019t have anything publicly posted about it, but have been using it for quite a while in production. San Francisco, CA and Ann Arbor, MI December 2018 TailorMed production We\u2019re deploying all of our platform and all K8s resources using Helmfile. The power of Helmfile are more then amazing. Thank you! Tel-Aviv, Israel September 2020 VSHN \u2013 The DevOps Company production Zurich, Switzerland March 2019 Vlocity proof-of-concept Melbourne, Australia March 2019 transit production Blog post . Montreal, Canada March 2019 uniqkey production Wiki Page Copenhagen, Denmark April 2019 bitsofinfo production Used with helmfile-deploy to manage releases for dozens of apps USA July 2019 kloeckner-i production We are deploying our standard tools via helmfile to all our clusters. Berlin, Germany September 2019 American Express proof-of-concept Orchestration of both internal cluster workloads and local developer environments. London, GB January 2020 Sportradar production Since mid-2019, we\u2019ve been deploying our core infrastructure and several application stacks with Helmfile. St. Gallen, Switzerland March 2020 PedidosYa production Montevideo, Uruguay June 2020 Jenkins OSS production jenkins-infra/charts * July 2020 SettleMint production The SettleMint platform allows enterprises to spin up k8s clusters and deploy production grade blockchain networks and additional services. Helmfile is in charge of deploying these networks and services on demand out of the self service management ui. Belgium, Singapore, UAE, India October 2020 AutoTrader (UK) production We\u2019ve used Helmfile for 2+ years to deploy 400+ services UK October 2020 Trend Micro production We manage 9 k8s clusters in a mono git repository with Helmfile Taipei, Taiwan December 2019 William Hill production We have standardised on Helmfile for managing both our application services and our Kubernetes platform components. International March 2021 The Hyve production We\u2019ve been using Helmfile since 2019 to deploy Radar-base applications . Blog Post Utrecht, Netherlands April 2021","title":"Users"},{"location":"USERS/#helmfile-users","text":"Who\u2019s actually using Helmfile? The following table is compiled from public information, survey response and PRs. It\u2019s likely this is only a small selection of actual users, but it\u2019s a start. If you are using Helmfile in your company or organization, we would like to invite you to create a PR to add your information to this file. Organization Workload More Info Location Added reddit.com production GitHub issue , Talk San Francisco, CA April 2018 ricardo.ch production We\u2019re deploying our complete application platform using Helmfile. Zug, Switzerland April 2018 Hellofresh production We\u2019ve been using helmfile in production since April 2018 for deploying all our infrastructure applications Berlin, Germany April 2018 Cherre production We have no public posts about using Helmfile, but we have been using it for a long time now New York, NY October 2018 Sight Machine production We don\u2019t have anything publicly posted about it, but have been using it for quite a while in production. San Francisco, CA and Ann Arbor, MI December 2018 TailorMed production We\u2019re deploying all of our platform and all K8s resources using Helmfile. The power of Helmfile are more then amazing. Thank you! Tel-Aviv, Israel September 2020 VSHN \u2013 The DevOps Company production Zurich, Switzerland March 2019 Vlocity proof-of-concept Melbourne, Australia March 2019 transit production Blog post . Montreal, Canada March 2019 uniqkey production Wiki Page Copenhagen, Denmark April 2019 bitsofinfo production Used with helmfile-deploy to manage releases for dozens of apps USA July 2019 kloeckner-i production We are deploying our standard tools via helmfile to all our clusters. Berlin, Germany September 2019 American Express proof-of-concept Orchestration of both internal cluster workloads and local developer environments. London, GB January 2020 Sportradar production Since mid-2019, we\u2019ve been deploying our core infrastructure and several application stacks with Helmfile. St. Gallen, Switzerland March 2020 PedidosYa production Montevideo, Uruguay June 2020 Jenkins OSS production jenkins-infra/charts * July 2020 SettleMint production The SettleMint platform allows enterprises to spin up k8s clusters and deploy production grade blockchain networks and additional services. Helmfile is in charge of deploying these networks and services on demand out of the self service management ui. Belgium, Singapore, UAE, India October 2020 AutoTrader (UK) production We\u2019ve used Helmfile for 2+ years to deploy 400+ services UK October 2020 Trend Micro production We manage 9 k8s clusters in a mono git repository with Helmfile Taipei, Taiwan December 2019 William Hill production We have standardised on Helmfile for managing both our application services and our Kubernetes platform components. International March 2021 The Hyve production We\u2019ve been using Helmfile since 2019 to deploy Radar-base applications . Blog Post Utrecht, Netherlands April 2021","title":"Helmfile Users"},{"location":"advanced-features/","text":"Advanced Features \u00b6 Import Configuration Parameters into Helmfile Deploy Kustomization with Helmfile Adhoc Kustomization of Helm Charts Adding dependencies without forking the chart Import Configuration Parameters into Helmfile \u00b6 Helmfile integrates vals to import configuration parameters from following backends: AWS SSM Parameter Store AWS SecretsManager Vault SOPS See Vals \u201cSuported Backends\u201d for the full list of available backends. This feature was implemented in https://github.com/roboll/helmfile/pull/906. If you\u2019re curious how it\u2019s designed and how it works, please consult the pull request. Deploy Kustomizations with Helmfile \u00b6 You can deploy kustomize \u201ckustomization\u201ds with Helmfile. Most of Kustomize operations that is usually done with kustomize edit can be done declaratively via Helm values.yaml files. Under the hood, Helmfile transforms the kustomization into a local chart in a temporary directory so that it can be helm upgrade --install ed. The transformation is done by generating (1)a temporary kustomization from various options and (2)temporary chart from the temporary kustomization. An example pseudo code for the transformation logic can be written as: $ TMPCHART=/tmp/sometmpdir $ mkdir -p ${TMPCHART}/templates $ somehow_generate_chart_yaml ${TMPCHART}/Chart.yaml $ TMPKUSTOMIZATION=/tmp/sometmpdir2 $ somehow_generate_temp_kustomization_yaml ${TMPKUSTOMIZATION}/kustomization.yaml $ kustomize build ${TMPKUSTOMIZATION}/kustomization.yaml > ${TMPCHART}/templates/all.yaml Let\u2019s say you have a helmfile.yaml that looks like the below: releases: - name: myapp chart: mykustomization values: - values.yaml Helmfile firstly generates a temporary kustomization.yaml that looks like: bases: - $(ABS_PATH_TO_HELMFILE_YAML}/mykustomization Followed by the below steps: Running kustomize edit set image $IMAGE for every $IMAGE generated from your values.yaml Running kustomize edit set nameprefix $NAMEPREFIX with the nameprefix specified in your values.yaml Running kustomize edit set namesuffix $NAMESUFFIX with the namesuffix specified in your values.yaml Running kustomize edit set namespace $NS with the namespace specified in your values.yaml A values.yaml file for kustomization would look like the below: images: # kustomize edit set image mysql=eu.gcr.io/my-project/mysql@canary - name: mysql newName: eu.gcr.io/my-project/mysql newTag: canary # kustomize edit set image myapp=my-registry/my-app@sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 - name: myapp digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 newName: my-registry/my-app # kustomize edit set nameprefix foo- namePrefix: foo- # kustomize edit set namesuffix -bar nameSuffix: -bar # kustomize edit set namespace myapp namespace: myapp At this point, Helmfile can generate a complete kustomization from the base kustomization you specified in releases[].chart of your helmfile.yaml and values.yaml , which can be included in the temporary chart. After all, Helmfile just installs the temporary chart like standard charts, which allows you to manage everything with Helmfile regardless of each app is declared using a Helm chart or a kustomization. Please also see test/advanced/helmfile.yaml for an example of kustomization support and more. Adhoc Kustomization of Helm charts \u00b6 With Helmfile\u2019s integration with Helmfile, not only deploying Kustomization as a Helm chart, you can kustomize charts before installation. Currently, Helmfile allows you to set the following fields for kustomizing the chart: releases[].strategicMergePatches releases[].jsonPatches releases[].transformers strategicMergePatches \u00b6 You can add/update any Kubernetes resource field rendered from a Helm chart by specifying releases[].strategicMergePatches : repositories: - name: incubator url: https://kubernetes-charts-incubator.storage.googleapis.com releases: - name: raw1 chart: incubator/raw values: - resources: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: foo: FOO strategicMergePatches: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: bar: BAR Running helmfile template on the above example results in a ConfigMap called raw whose data is: foo: FOO bar: BAR Please note that the second data field bar is coming from the strategic-merge patch defined in the above helmfile.yaml. There\u2019s also releases[].jsonPatches that works similarly to strategicMergePatches but has additional capability to remove fields. Please also see test/advanced/helmfile.yaml for an example of patching support and more. transformers \u00b6 You can set transformers to apply Kustomize\u2019s transformers . Each item can be a path to a YAML or go template file, or an embedded transformer declaration as a YAML hash. It\u2019s often used to add common labels and annotations to your resources. In the below example. we add common annotations and labels every resource rendered from the aws-load-balancer-controller chart: releases: - name: \"aws-load-balancer-controller\" namespace: \"kube-system\" forceNamespace: \"kube-system\" chart: \"center/aws/aws-load-balancer-controller\" transformers: - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true - apiVersion: builtin kind: LabelTransformer metadata: name: notImportantHere labels: foo: bar fieldSpecs: - path: metadata/labels create: true As explained earlier, transformers can be not only a list of embedded transformers, but also YAML or go template files, or a mix of those three kinds. transformers: # Embedded transformer - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true # YAML file - path/to/transformer.yaml # Go template # The same set of template parameters as release values files templates is available. - path/to/transformer.yaml.gotmpl Please see https://github.com/kubernetes-sigs/kustomize/blob/master/examples/configureBuiltinPlugin.md#configuring-the-builtin-plugins-instead for more information on how to declare transformers. Adding dependencies without forking the chart \u00b6 With Helmfile, you can add chart dependencies to a Helm chart without forking it. An example helmfile.yaml that adds a stable/envoy dependency to the release foo looks like the below: repositories: - name: stable url: https://charts.helm.sh/stable releases: - name: foo chart: ./path/to/foo dependencies: - chart: stable/envoy version: 1.5 When Helmfile encounters releases[].dependencies , it creates a another temporary chart from ./path/to/foo and adds the following dependencies to the Chart.yaml , so that you don\u2019t need to fork the chart. dependencies: - name: envoy repo: https://charts.helm.sh/stable condition: envoy.enabled A Helm chart can have two or more dependencies for the same chart with different alias es. To give your dependency an alias , defien it like you would do in a standard Chart.yaml : repositories: - name: stable url: https://charts.helm.sh/stable releases: - name: foo chart: ./path/to/foo dependencies: - chart: stable/envoy version: 1.5 alias: bar - chart: stable/envoy version: 1.5 alias: baz which will tweaks the temporary chart\u2019s Chart.yaml to have: dependencies: - alias: bar name: envoy repo: https://charts.helm.sh/stable condition: bar.enabled - alias: baz name: envoy repo: https://charts.helm.sh/stable condition: baz.enabled Please see #649 for more context around this feature. After the support for adhoc dependency to local chart (#1765), you can even write local file paths relative to helmfile.yaml in chart : releases: - name: foo chart: ./path/to/foo dependencies: - chart: ./path/to/bar Internally, Helmfile creates another temporary chart from the local chart ./path/to/foo , and modifies the chart\u2019s Chart.yaml dependencies to look like: dependencies: - alias: bar name: bar repo: file:///abs/path/to/bar condition: bar.enabled Please read https://github.com/roboll/helmfile/issues/1762#issuecomment-816341251 for more details.","title":"Advanced Features"},{"location":"advanced-features/#advanced-features","text":"Import Configuration Parameters into Helmfile Deploy Kustomization with Helmfile Adhoc Kustomization of Helm Charts Adding dependencies without forking the chart","title":"Advanced Features"},{"location":"advanced-features/#import-configuration-parameters-into-helmfile","text":"Helmfile integrates vals to import configuration parameters from following backends: AWS SSM Parameter Store AWS SecretsManager Vault SOPS See Vals \u201cSuported Backends\u201d for the full list of available backends. This feature was implemented in https://github.com/roboll/helmfile/pull/906. If you\u2019re curious how it\u2019s designed and how it works, please consult the pull request.","title":"Import Configuration Parameters into Helmfile"},{"location":"advanced-features/#deploy-kustomizations-with-helmfile","text":"You can deploy kustomize \u201ckustomization\u201ds with Helmfile. Most of Kustomize operations that is usually done with kustomize edit can be done declaratively via Helm values.yaml files. Under the hood, Helmfile transforms the kustomization into a local chart in a temporary directory so that it can be helm upgrade --install ed. The transformation is done by generating (1)a temporary kustomization from various options and (2)temporary chart from the temporary kustomization. An example pseudo code for the transformation logic can be written as: $ TMPCHART=/tmp/sometmpdir $ mkdir -p ${TMPCHART}/templates $ somehow_generate_chart_yaml ${TMPCHART}/Chart.yaml $ TMPKUSTOMIZATION=/tmp/sometmpdir2 $ somehow_generate_temp_kustomization_yaml ${TMPKUSTOMIZATION}/kustomization.yaml $ kustomize build ${TMPKUSTOMIZATION}/kustomization.yaml > ${TMPCHART}/templates/all.yaml Let\u2019s say you have a helmfile.yaml that looks like the below: releases: - name: myapp chart: mykustomization values: - values.yaml Helmfile firstly generates a temporary kustomization.yaml that looks like: bases: - $(ABS_PATH_TO_HELMFILE_YAML}/mykustomization Followed by the below steps: Running kustomize edit set image $IMAGE for every $IMAGE generated from your values.yaml Running kustomize edit set nameprefix $NAMEPREFIX with the nameprefix specified in your values.yaml Running kustomize edit set namesuffix $NAMESUFFIX with the namesuffix specified in your values.yaml Running kustomize edit set namespace $NS with the namespace specified in your values.yaml A values.yaml file for kustomization would look like the below: images: # kustomize edit set image mysql=eu.gcr.io/my-project/mysql@canary - name: mysql newName: eu.gcr.io/my-project/mysql newTag: canary # kustomize edit set image myapp=my-registry/my-app@sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 - name: myapp digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 newName: my-registry/my-app # kustomize edit set nameprefix foo- namePrefix: foo- # kustomize edit set namesuffix -bar nameSuffix: -bar # kustomize edit set namespace myapp namespace: myapp At this point, Helmfile can generate a complete kustomization from the base kustomization you specified in releases[].chart of your helmfile.yaml and values.yaml , which can be included in the temporary chart. After all, Helmfile just installs the temporary chart like standard charts, which allows you to manage everything with Helmfile regardless of each app is declared using a Helm chart or a kustomization. Please also see test/advanced/helmfile.yaml for an example of kustomization support and more.","title":"Deploy Kustomizations with Helmfile"},{"location":"advanced-features/#adhoc-kustomization-of-helm-charts","text":"With Helmfile\u2019s integration with Helmfile, not only deploying Kustomization as a Helm chart, you can kustomize charts before installation. Currently, Helmfile allows you to set the following fields for kustomizing the chart: releases[].strategicMergePatches releases[].jsonPatches releases[].transformers","title":"Adhoc Kustomization of Helm charts"},{"location":"advanced-features/#strategicmergepatches","text":"You can add/update any Kubernetes resource field rendered from a Helm chart by specifying releases[].strategicMergePatches : repositories: - name: incubator url: https://kubernetes-charts-incubator.storage.googleapis.com releases: - name: raw1 chart: incubator/raw values: - resources: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: foo: FOO strategicMergePatches: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: bar: BAR Running helmfile template on the above example results in a ConfigMap called raw whose data is: foo: FOO bar: BAR Please note that the second data field bar is coming from the strategic-merge patch defined in the above helmfile.yaml. There\u2019s also releases[].jsonPatches that works similarly to strategicMergePatches but has additional capability to remove fields. Please also see test/advanced/helmfile.yaml for an example of patching support and more.","title":"strategicMergePatches"},{"location":"advanced-features/#transformers","text":"You can set transformers to apply Kustomize\u2019s transformers . Each item can be a path to a YAML or go template file, or an embedded transformer declaration as a YAML hash. It\u2019s often used to add common labels and annotations to your resources. In the below example. we add common annotations and labels every resource rendered from the aws-load-balancer-controller chart: releases: - name: \"aws-load-balancer-controller\" namespace: \"kube-system\" forceNamespace: \"kube-system\" chart: \"center/aws/aws-load-balancer-controller\" transformers: - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true - apiVersion: builtin kind: LabelTransformer metadata: name: notImportantHere labels: foo: bar fieldSpecs: - path: metadata/labels create: true As explained earlier, transformers can be not only a list of embedded transformers, but also YAML or go template files, or a mix of those three kinds. transformers: # Embedded transformer - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true # YAML file - path/to/transformer.yaml # Go template # The same set of template parameters as release values files templates is available. - path/to/transformer.yaml.gotmpl Please see https://github.com/kubernetes-sigs/kustomize/blob/master/examples/configureBuiltinPlugin.md#configuring-the-builtin-plugins-instead for more information on how to declare transformers.","title":"transformers"},{"location":"advanced-features/#adding-dependencies-without-forking-the-chart","text":"With Helmfile, you can add chart dependencies to a Helm chart without forking it. An example helmfile.yaml that adds a stable/envoy dependency to the release foo looks like the below: repositories: - name: stable url: https://charts.helm.sh/stable releases: - name: foo chart: ./path/to/foo dependencies: - chart: stable/envoy version: 1.5 When Helmfile encounters releases[].dependencies , it creates a another temporary chart from ./path/to/foo and adds the following dependencies to the Chart.yaml , so that you don\u2019t need to fork the chart. dependencies: - name: envoy repo: https://charts.helm.sh/stable condition: envoy.enabled A Helm chart can have two or more dependencies for the same chart with different alias es. To give your dependency an alias , defien it like you would do in a standard Chart.yaml : repositories: - name: stable url: https://charts.helm.sh/stable releases: - name: foo chart: ./path/to/foo dependencies: - chart: stable/envoy version: 1.5 alias: bar - chart: stable/envoy version: 1.5 alias: baz which will tweaks the temporary chart\u2019s Chart.yaml to have: dependencies: - alias: bar name: envoy repo: https://charts.helm.sh/stable condition: bar.enabled - alias: baz name: envoy repo: https://charts.helm.sh/stable condition: baz.enabled Please see #649 for more context around this feature. After the support for adhoc dependency to local chart (#1765), you can even write local file paths relative to helmfile.yaml in chart : releases: - name: foo chart: ./path/to/foo dependencies: - chart: ./path/to/bar Internally, Helmfile creates another temporary chart from the local chart ./path/to/foo , and modifies the chart\u2019s Chart.yaml dependencies to look like: dependencies: - alias: bar name: bar repo: file:///abs/path/to/bar condition: bar.enabled Please read https://github.com/roboll/helmfile/issues/1762#issuecomment-816341251 for more details.","title":"Adding dependencies without forking the chart"},{"location":"remote-secrets/","text":"Secrets \u00b6 helmfile can handle secrets using helm-secrets plugin or using remote secrets storage (everything that package vals can handle vault, AWS SSM etc) This section will describe the second use case. Remote secrets \u00b6 This paragraph will describe how to use remote secrets storage (vault, SSM etc) in helmfile Fetching single key \u00b6 To fetch single key from remote secret storage you can use fetchSecretValue template function example below # helmfile.yaml repositories: - name: stable url: https://kubernetes-charts.storage.googleapis.com environments: default: values: - service: password: ref+vault://svc/#pass login: ref+vault://svc/#login releases: - name: service namespace: default labels: cluster: services secrets: vault chart: stable/svc version: 0.1.0 values: - service: login: {{ .Values.service.login | fetchSecretValue }} # this will resolve ref+vault://svc/#pass and fetch secret from vault password: {{ .Values.service.password | fetchSecretValue | quote }} # - values/service.yaml.gotmpl # alternatively Fetching multiple keys \u00b6 Alternatively you can use expandSecretRefs to fetch a map of secrets # values/service.yaml.gotmpl service: {{ .Values.service | expandSecretRefs | toYaml | nindent 2 }} This will produce # values/service.yaml service: login: svc-login # fetched from vault password: pass","title":"Secrets"},{"location":"remote-secrets/#secrets","text":"helmfile can handle secrets using helm-secrets plugin or using remote secrets storage (everything that package vals can handle vault, AWS SSM etc) This section will describe the second use case.","title":"Secrets"},{"location":"remote-secrets/#remote-secrets","text":"This paragraph will describe how to use remote secrets storage (vault, SSM etc) in helmfile","title":"Remote secrets"},{"location":"remote-secrets/#fetching-single-key","text":"To fetch single key from remote secret storage you can use fetchSecretValue template function example below # helmfile.yaml repositories: - name: stable url: https://kubernetes-charts.storage.googleapis.com environments: default: values: - service: password: ref+vault://svc/#pass login: ref+vault://svc/#login releases: - name: service namespace: default labels: cluster: services secrets: vault chart: stable/svc version: 0.1.0 values: - service: login: {{ .Values.service.login | fetchSecretValue }} # this will resolve ref+vault://svc/#pass and fetch secret from vault password: {{ .Values.service.password | fetchSecretValue | quote }} # - values/service.yaml.gotmpl # alternatively","title":"Fetching single key"},{"location":"remote-secrets/#fetching-multiple-keys","text":"Alternatively you can use expandSecretRefs to fetch a map of secrets # values/service.yaml.gotmpl service: {{ .Values.service | expandSecretRefs | toYaml | nindent 2 }} This will produce # values/service.yaml service: login: svc-login # fetched from vault password: pass","title":"Fetching multiple keys"},{"location":"shared-configuration-across-teams/","text":"Shared Configuration Across Teams \u00b6 Assume you have a two or more teams, each works for a different internal or external service, like: Product 1 Product 2 Observability The simplest helmfile.yaml that declares the whole cluster that is composed of the three services would look like the below: releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: product2-api chart: saas-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip This works, but what if you wanted to a separate cluster per service to achieve smaller blast radius? Let\u2019s start by creating a helmfile.yaml for each service. product1/helmfile.yaml : releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip product2/helmfile.yaml : releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip You will (of course!) notice this isn\u2019t DRY. To remove the duplication of observability stack between the two helmfiles, create a \u201csub-helmfile\u201d for the observability stack. observability/helmfile.yaml : - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip As you might have imagined, the observability helmfile can be reused from the two product helmfiles by declaring helmfiles . product1/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip product2/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip Using sub-helmfile as a template \u00b6 You can go even further by generalizing the product related releases as a pair of api and web : shared/helmfile.yaml : releases: - name: product{{ env \"PRODUCT_ID\" }}-api chart: product{{ env \"PRODUCT_ID\" }}-charts/api # snip - name: product{{ env \"PRODUCT_ID\" }}-web chart: product{{ env \"PRODUCT_ID\" }}-charts/web # snip Then you only need one single product helmfile product/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml - ../shared/helmfile.yaml Now that we use the environment variable PRODUCT_ID to as the parameters of release names, you need to set it before running helmfile , so that it produces the differently named releases per product: $ PRODUCT_ID=1 helmfile -f product/helmfile.yaml apply $ PRODUCT_ID=2 helmfile -f product/helmfile.yaml apply","title":"Shared Configuration Across Teams"},{"location":"shared-configuration-across-teams/#shared-configuration-across-teams","text":"Assume you have a two or more teams, each works for a different internal or external service, like: Product 1 Product 2 Observability The simplest helmfile.yaml that declares the whole cluster that is composed of the three services would look like the below: releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: product2-api chart: saas-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip This works, but what if you wanted to a separate cluster per service to achieve smaller blast radius? Let\u2019s start by creating a helmfile.yaml for each service. product1/helmfile.yaml : releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip product2/helmfile.yaml : releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip You will (of course!) notice this isn\u2019t DRY. To remove the duplication of observability stack between the two helmfiles, create a \u201csub-helmfile\u201d for the observability stack. observability/helmfile.yaml : - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip As you might have imagined, the observability helmfile can be reused from the two product helmfiles by declaring helmfiles . product1/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip product2/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip","title":"Shared Configuration Across Teams"},{"location":"shared-configuration-across-teams/#using-sub-helmfile-as-a-template","text":"You can go even further by generalizing the product related releases as a pair of api and web : shared/helmfile.yaml : releases: - name: product{{ env \"PRODUCT_ID\" }}-api chart: product{{ env \"PRODUCT_ID\" }}-charts/api # snip - name: product{{ env \"PRODUCT_ID\" }}-web chart: product{{ env \"PRODUCT_ID\" }}-charts/web # snip Then you only need one single product helmfile product/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml - ../shared/helmfile.yaml Now that we use the environment variable PRODUCT_ID to as the parameters of release names, you need to set it before running helmfile , so that it produces the differently named releases per product: $ PRODUCT_ID=1 helmfile -f product/helmfile.yaml apply $ PRODUCT_ID=2 helmfile -f product/helmfile.yaml apply","title":"Using sub-helmfile as a template"},{"location":"writing-helmfile/","text":"The Helmfile Best Practices Guide \u00b6 This guide covers the Helmfile\u2019s considered patterns for writing advanced helmfiles. It focuses on how helmfile should be structured and executed. Helmfile .Values vs Helm .Values \u00b6 Templating engine of Helmfile uses the same pipeline name .Values as Helm, so in some use-cases .Values of Helmfile and Helm can be seen in the same file. To distinguish these two kinds of .Values , Helmfile provides an alias .StateValues for its .Values . app: project: {{.Environmont.Name}}-{{.StateValues.project}} # Same as {{.Environmont.Name}}-{{.Values.project}} {{` extraEnvVars: - name: APP_PROJECT value: {{.Values.app.project}} `}} Missing keys and Default values \u00b6 helmfile tries its best to inform users for noticing potential mistakes. One example of how helmfile achieves it is that, helmfile fails when you tried to access missing keys in environment values. That is, the following example let helmfile fail when you have no eventApi.replicas defined in environment values. {{ .Values.eventApi.replicas | default 1 }} In case it isn\u2019t a mistake and you do want to allow missing keys, use the get template function: {{ .Values | get \"eventApi.replicas\" nil }} This result in printing <no value in your template, that may or may not result in a failure. If you want a kind of default values that is used when a missing key was referenced, use default like: {{ .Values | get \"eventApi.replicas\" 1 }} Now, you get 1 when there is no eventApi.replicas defined in environment values. Release Template / Conventional Directory Structure \u00b6 Introducing helmfile into a large-scale project that involves dozens of releases often results in a lot of repetitions in helmfile.yaml files. The example below shows repetitions in namespace , chart , values , and secrets : releases: # *snip* - name: heapster namespace: kube-system chart: stable/heapster version: 0.3.2 values: - \"./config/heapster/values.yaml\" - \"./config/heapster/{{ .Environment.Name }}.yaml\" secrets: - \"./config/heapster/secrets.yaml\" - \"./config/heapster/{{ .Environment.Name }}-secrets.yaml\" - name: kubernetes-dashboard namespace: kube-system chart: stable/kubernetes-dashboard version: 0.10.0 values: - \"./config/kubernetes-dashboard/values.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}.yaml\" secrets: - \"./config/kubernetes-dashboard/secrets.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}-secrets.yaml\" This is where Helmfile\u2019s advanced feature called Release Template comes handy. It allows you to abstract away the repetitions in releases into a template, which is then included and executed by using YAML anchor/alias: templates: default: &default chart: stable/{{`{{ .Release.Name }}`}} namespace: kube-system # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler: Warn values: - config/{{`{{ .Release.Name }}`}}/values.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}-secrets.yaml releases: - name: heapster <<: *default - name: kubernetes-dashboard <<: *default Release Templating supports the following parts of release definition: - basic fields: name , namespace , chart , version - boolean fields: installed , wait , waitForJobs , tillerless , verify by the means of additional text fields designed for templating only: installedTemplate , waitTemplate , tillerlessTemplate , verifyTemplate yaml # ... installedTemplate: '{{`{{ eq .Release.Namespace \"kube-system\" }}`}}' waitTemplate: '{{`{{ eq .Release.Labels.tag \"safe\" | not }}`}}' # ... - set block values: yaml # ... setTemplate: - name: '{{`{{ .Release.Name }}`}}' values: '{{`{{ .Release.Namespace }}`}}' # ... - values and secrets file paths: yaml # ... valuesTemplate: - config/{{`{{ .Release.Name }}`}}/values.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml # ... - inline values map: yaml # ... valuesTemplate: - image: tag: `{{ .Release.Labels.tag }}` # ... See the issue 428 for more context on how this is supposed to work. Layering Release Values \u00b6 Please note, that it is not possible to layer values sections. If values is defined in the release and in the release template, only the values defined in the release will be considered. The same applies to secrets and set . Layering State Files \u00b6 See Layering State Template Files if you\u2019re layering templates. You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfile s, so that each helmfile becomes DRY. Let\u2019s assume that your helmfile.yaml looks like: bases: - environments.yaml releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Whereas environments.yaml contained well-known environments: environments: development: production: At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments: development: production: --- # helmfile.yaml releases: - name: myapp chart: mychart - name: metricbeat chart: stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments: development: production: releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Great! Now, repeat the above steps for each your helmfile.yaml , so that all your helmfiles becomes DRY. Please also see the discussion in the issue 388 for more advanced layering examples. Merging Arrays in Layers \u00b6 Helmfile doesn\u2019t merge arrays across layers. That is, the below example doesn\u2019t work as you might have expected: releases: - name: metricbeat chart: stable/metricbeat --- releases: - name: myapp chart: mychart Helmfile overrides the releases array with the latest layer so the resulting state file will be: releases: # metricbeat release disappeared! but that's how helmfile works - name: myapp chart: mychart A work-around is to treat the state file as a go template and use readFile template function to import the common part of your state file as a plain text: common.yaml : templates: metricbeat: &metricbeat name: metricbeat chart: stable/metricbeat helmfile.yaml : {{ readFile \"common.yaml\" }} releases: - <<: *metricbeat - name: myapp chart: mychart Layering State Template Files \u00b6 Do you need to make your state file even more DRY? Turned out layering state files wasn\u2019t enough for you? Helmfile supports an advanced feature that allows you to compose state \u201ctemplate\u201d files to generate the final state to be processed. In the following example helmfile.yaml.gotmpl , each --- separated part of the file is a go template. helmfile.yaml.gotmpl : # Part 1: Reused Enviroment Values bases: - myenv.yaml --- # Part 2: Reused Defaults bases: - mydefaults.yaml --- # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" Suppose the myenv.yaml and test.env.yaml loaded in the first part looks like: myenv.yaml : environments: test: values: - test.env.yaml test.env.yaml : kubeContext: test wait: false cvOnly: false myname: \"dog\" Where the gotmpl file loaded in the second part looks like: mydefaults.yaml.gotmpl : helmDefaults: tillerNamespace: kube-system kubeContext: {{ .Values.kubeContext }} verify: false {{ if .Values.wait }} wait: true {{ else }} wait: false {{ end }} timeout: 600 recreatePods: false force: true Each go template is rendered in the context where .Values is inherited from the previous part. So in mydefaults.yaml.gotmpl , both .Values.kubeContext and .Values.wait are valid as they do exist in the environment values inherited from the previous part(=the first part) of your helmfile.yaml.gotmpl , and therefore the template is rendered to: helmDefaults: tillerNamespace: kube-system kubeContext: test verify: false wait: false timeout: 600 recreatePods: false force: true Similarly, the third part of the top-level helmfile.yaml.gotmpl , .Values.myname is valid as it is included in the environment values inherited from the previous parts: # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" ```` hence rendered to: ```yaml # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-dog values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\"","title":"Best Practices Guide"},{"location":"writing-helmfile/#the-helmfile-best-practices-guide","text":"This guide covers the Helmfile\u2019s considered patterns for writing advanced helmfiles. It focuses on how helmfile should be structured and executed.","title":"The Helmfile Best Practices Guide"},{"location":"writing-helmfile/#helmfile-values-vs-helm-values","text":"Templating engine of Helmfile uses the same pipeline name .Values as Helm, so in some use-cases .Values of Helmfile and Helm can be seen in the same file. To distinguish these two kinds of .Values , Helmfile provides an alias .StateValues for its .Values . app: project: {{.Environmont.Name}}-{{.StateValues.project}} # Same as {{.Environmont.Name}}-{{.Values.project}} {{` extraEnvVars: - name: APP_PROJECT value: {{.Values.app.project}} `}}","title":"Helmfile .Values vs Helm .Values"},{"location":"writing-helmfile/#missing-keys-and-default-values","text":"helmfile tries its best to inform users for noticing potential mistakes. One example of how helmfile achieves it is that, helmfile fails when you tried to access missing keys in environment values. That is, the following example let helmfile fail when you have no eventApi.replicas defined in environment values. {{ .Values.eventApi.replicas | default 1 }} In case it isn\u2019t a mistake and you do want to allow missing keys, use the get template function: {{ .Values | get \"eventApi.replicas\" nil }} This result in printing <no value in your template, that may or may not result in a failure. If you want a kind of default values that is used when a missing key was referenced, use default like: {{ .Values | get \"eventApi.replicas\" 1 }} Now, you get 1 when there is no eventApi.replicas defined in environment values.","title":"Missing keys and Default values"},{"location":"writing-helmfile/#release-template-conventional-directory-structure","text":"Introducing helmfile into a large-scale project that involves dozens of releases often results in a lot of repetitions in helmfile.yaml files. The example below shows repetitions in namespace , chart , values , and secrets : releases: # *snip* - name: heapster namespace: kube-system chart: stable/heapster version: 0.3.2 values: - \"./config/heapster/values.yaml\" - \"./config/heapster/{{ .Environment.Name }}.yaml\" secrets: - \"./config/heapster/secrets.yaml\" - \"./config/heapster/{{ .Environment.Name }}-secrets.yaml\" - name: kubernetes-dashboard namespace: kube-system chart: stable/kubernetes-dashboard version: 0.10.0 values: - \"./config/kubernetes-dashboard/values.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}.yaml\" secrets: - \"./config/kubernetes-dashboard/secrets.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}-secrets.yaml\" This is where Helmfile\u2019s advanced feature called Release Template comes handy. It allows you to abstract away the repetitions in releases into a template, which is then included and executed by using YAML anchor/alias: templates: default: &default chart: stable/{{`{{ .Release.Name }}`}} namespace: kube-system # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler: Warn values: - config/{{`{{ .Release.Name }}`}}/values.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}-secrets.yaml releases: - name: heapster <<: *default - name: kubernetes-dashboard <<: *default Release Templating supports the following parts of release definition: - basic fields: name , namespace , chart , version - boolean fields: installed , wait , waitForJobs , tillerless , verify by the means of additional text fields designed for templating only: installedTemplate , waitTemplate , tillerlessTemplate , verifyTemplate yaml # ... installedTemplate: '{{`{{ eq .Release.Namespace \"kube-system\" }}`}}' waitTemplate: '{{`{{ eq .Release.Labels.tag \"safe\" | not }}`}}' # ... - set block values: yaml # ... setTemplate: - name: '{{`{{ .Release.Name }}`}}' values: '{{`{{ .Release.Namespace }}`}}' # ... - values and secrets file paths: yaml # ... valuesTemplate: - config/{{`{{ .Release.Name }}`}}/values.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml # ... - inline values map: yaml # ... valuesTemplate: - image: tag: `{{ .Release.Labels.tag }}` # ... See the issue 428 for more context on how this is supposed to work.","title":"Release Template / Conventional Directory Structure"},{"location":"writing-helmfile/#layering-release-values","text":"Please note, that it is not possible to layer values sections. If values is defined in the release and in the release template, only the values defined in the release will be considered. The same applies to secrets and set .","title":"Layering Release Values"},{"location":"writing-helmfile/#layering-state-files","text":"See Layering State Template Files if you\u2019re layering templates. You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfile s, so that each helmfile becomes DRY. Let\u2019s assume that your helmfile.yaml looks like: bases: - environments.yaml releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Whereas environments.yaml contained well-known environments: environments: development: production: At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments: development: production: --- # helmfile.yaml releases: - name: myapp chart: mychart - name: metricbeat chart: stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments: development: production: releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Great! Now, repeat the above steps for each your helmfile.yaml , so that all your helmfiles becomes DRY. Please also see the discussion in the issue 388 for more advanced layering examples.","title":"Layering State Files"},{"location":"writing-helmfile/#merging-arrays-in-layers","text":"Helmfile doesn\u2019t merge arrays across layers. That is, the below example doesn\u2019t work as you might have expected: releases: - name: metricbeat chart: stable/metricbeat --- releases: - name: myapp chart: mychart Helmfile overrides the releases array with the latest layer so the resulting state file will be: releases: # metricbeat release disappeared! but that's how helmfile works - name: myapp chart: mychart A work-around is to treat the state file as a go template and use readFile template function to import the common part of your state file as a plain text: common.yaml : templates: metricbeat: &metricbeat name: metricbeat chart: stable/metricbeat helmfile.yaml : {{ readFile \"common.yaml\" }} releases: - <<: *metricbeat - name: myapp chart: mychart","title":"Merging Arrays in Layers"},{"location":"writing-helmfile/#layering-state-template-files","text":"Do you need to make your state file even more DRY? Turned out layering state files wasn\u2019t enough for you? Helmfile supports an advanced feature that allows you to compose state \u201ctemplate\u201d files to generate the final state to be processed. In the following example helmfile.yaml.gotmpl , each --- separated part of the file is a go template. helmfile.yaml.gotmpl : # Part 1: Reused Enviroment Values bases: - myenv.yaml --- # Part 2: Reused Defaults bases: - mydefaults.yaml --- # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" Suppose the myenv.yaml and test.env.yaml loaded in the first part looks like: myenv.yaml : environments: test: values: - test.env.yaml test.env.yaml : kubeContext: test wait: false cvOnly: false myname: \"dog\" Where the gotmpl file loaded in the second part looks like: mydefaults.yaml.gotmpl : helmDefaults: tillerNamespace: kube-system kubeContext: {{ .Values.kubeContext }} verify: false {{ if .Values.wait }} wait: true {{ else }} wait: false {{ end }} timeout: 600 recreatePods: false force: true Each go template is rendered in the context where .Values is inherited from the previous part. So in mydefaults.yaml.gotmpl , both .Values.kubeContext and .Values.wait are valid as they do exist in the environment values inherited from the previous part(=the first part) of your helmfile.yaml.gotmpl , and therefore the template is rendered to: helmDefaults: tillerNamespace: kube-system kubeContext: test verify: false wait: false timeout: 600 recreatePods: false force: true Similarly, the third part of the top-level helmfile.yaml.gotmpl , .Values.myname is valid as it is included in the environment values inherited from the previous parts: # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" ```` hence rendered to: ```yaml # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-dog values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\"","title":"Layering State Template Files"}]}